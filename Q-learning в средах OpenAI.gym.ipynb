{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Q-learning в средах OpenAI.gym</center></h1>\n",
    "\n",
    "### Евгений Пономарев\n",
    "\n",
    "### Сколковский институт науки и технологий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеки:\n",
    "* `pip install numpy`\n",
    "* `pip install tensorflow`\n",
    "* `pip install gym`\n",
    "* `pip install keras`\n",
    "\n",
    "ffmpeg\n",
    "Документация к openai.gym:\n",
    "* https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением aka Reinforcement learning:\n",
    "![](https://keras.io/img/keras-logo-small-wb.png)\n",
    "* Библиотека для машинного обучения (прежде всего, обучения нейронных сетей, в т.ч. глубоких). \n",
    "* Представляет собой удобную обертку для мощных и хорошо оптимизированных вычислительных библиотек: TensorFlow, Theano\n",
    "* Основные принципы: \n",
    "    1. Удобство использования\n",
    "    2. Модульность\n",
    "    3. Масштабируемость\n",
    "    4. Работа с Python\n",
    "    \n",
    "Инструмент с низким порогом входа, подходящий как продвинутым исследовалетям, так и энтузиастам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras: краткий обзор\n",
    "![](https://keras.io/img/keras-logo-small-wb.png)\n",
    "* Библиотека для машинного обучения (прежде всего, обучения нейронных сетей, в т.ч. глубоких). \n",
    "* Представляет собой удобную обертку для мощных и хорошо оптимизированных вычислительных библиотек: TensorFlow, Theano\n",
    "* Основные принципы: \n",
    "    1. Удобство использования\n",
    "    2. Модульность\n",
    "    3. Масштабируемость\n",
    "    4. Работа с Python\n",
    "    \n",
    "Инструмент с низким порогом входа, подходящий как продвинутым исследовалетям, так и энтузиастам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возвращение MNIST. Сравнения\n",
    "Давайте вернемся к старой доброй задаче распознавания рукописных цифр, чтобы посмотреть сколько занимает код для такой нейросети, написанный на keras. Кроме того, здесь же мы попробуем собрать простую сверточную сеть для улучшения результата.\n",
    "\n",
    "### Классическая нейросеть с плотными (dense) слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import keras.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка данных. В keras уже есть несколько популярных датасетов, которые можно легко загрузить. Давайте загрузим MNIST\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Подготовка датасета: нормализация значений на [0,1] и перевод признаков в one-hot формат\n",
    "X_train, X_test = X_train/255, X_test/255\n",
    "y_train, y_test = keras.utils.to_categorical(y_train, 10), keras.utils.to_categorical(y_test, 10)\n",
    "input_size = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создание модели. Sequential здесь означает последовательный тип модели, в который мы добавляем слои друг за другом\n",
    "model = Sequential()\n",
    "\n",
    "# Добавляем в стек модели слой за слоем. Полносвязный, активация и т.д.\n",
    "# Важно: в первом слое Sequential модели keras необходимо указать размерность входных данных.\n",
    "model.add(Flatten(input_shape=input_size))\n",
    "model.add(Dense(units=256, input_shape=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.6187 - acc: 0.8470     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.3332 - acc: 0.9079     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2851 - acc: 0.9205     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2539 - acc: 0.9290     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2297 - acc: 0.9355     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2751ac917f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После описания архитектуры необходимо скомпилировать модель, указав минимизируемую функцию потерь, \n",
    "# оптимизатор и попросив модель выводить точность работы на тестовой выорке в процессе обучения\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Тренировка с указанием данных, числа эпох и размера подвыборки\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21461834994256496, 0.93879999999999997]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим качество работы модели на тестовых данных. Выводится loss и точночть.\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим сверточный слой и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "# Создание модели. Sequential здесь означает последовательный тип модели, в который мы добавляем слои друг за другом\n",
    "conv_model = Sequential()\n",
    "\n",
    "# Добавим явно число каналов в наш датасет - это важно для сверточных слоев. \n",
    "# т.е. делается преобразование (60000, 28, 28) -> (60000, 28, 28, 1). Это ничего не изменяет.\n",
    "X_train, X_test = X_train.reshape((60000, 28, 28, 1)), X_test.reshape((10000, 28, 28, 1))\n",
    "input_size = X_train[0].shape\n",
    "\n",
    "# Здесь мы используем сверточный слой, который тренирует 32 фильтра размером 3x3 для поиска \n",
    "# конкретных геометрических (настраиваемых в процессе обучения)паттернов на входном изображении.\n",
    "conv_model.add(Conv2D(24, (3, 3), input_shape=input_size))\n",
    "conv_model.add(Activation('selu'))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(64, activation='selu'))\n",
    "conv_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10816/30000 [=========>....................] - ETA: 34s - loss: 0.4272 - acc: 0.8696"
     ]
    }
   ],
   "source": [
    "# Поставим другой оптимизатор для разнообразия\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Тренировка с указанием данных, числа эпох и размера подвыборки\n",
    "conv_model.fit(X_train[:30000], y_train[:30000], epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проверим качество работы модели на тестовых данных. Выводится loss и точночть.\n",
    "conv_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточные слои, dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируются и обучаются несколько фильтров небольших размеров так, чтобы распознавать какие то характерные сочетания пикселей (паттерны). Ниже на картинке изображение 5x5 пикселей, фильтр имеет размеры 3x3. При этом на выходе такой операции имеем картинку такого же размера, в каждый из пикселей которого записан результат свертки (число) данного фильтра с картинкой при нахождении центра фильтра в этом пикселе. Для этого исходную картинку необходимо дополнить по краям. Обычно это делают либо нулями, либо дублируют ближайшие пиксели (padding)\n",
    "![](convol.gif)\n",
    "\n",
    "Чем глубже сверточный слой, тем более сложные паттерны он способен распознавать:\n",
    "![](features.png)\n",
    "\n",
    "Dropout - техника спасения нейросетей от переобучения, при которой в процессе тренировки случайно \"выключаются\" некоторые нейроны из моделей.\n",
    "\n",
    "Альтернативный взгляд - вместо тренировки одной большой сети проходит одновременная тренировка нескольких подсетей меньшего размера, результаты которых потом усредняются (в каком то смысле, сглаживаются).\n",
    "![](dropout.gif)\n",
    "\n",
    "Давайте попробуем посмотреть, как написать сеть, состоящую из нескольких сверточных слоев на `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D, Dropout\n",
    "\n",
    "# Все так же, создаем модель\n",
    "cnn = Sequential()\n",
    "\n",
    "# Начинаем со сверточного слоя, указывая тип активации на выходе из него и способ заполнения краев (padding)\n",
    "cnn.add(Conv2D(64, (3, 3), input_shape=input_size, activation='selu', padding='same'))\n",
    "\n",
    "# Здесь мы используем метод MaxPooling, который уменьшает размер обрабатываемого изображения, \n",
    "# выбирая из 4 пикселей 1 с максимальным значением, чтобы это быстрее считалось. (2,2) -> 1\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Слой dropout, который на каждом шаге \"выключает\" 25% случайно выбранных нейронов\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "# Еще сверточный слой\n",
    "cnn.add(Conv2D(32, (3, 3), input_shape=input_size, activation='selu', padding='same'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "# Последний слой необходим для классификации, но перед ним необходимо векторизовать данные\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = 'nadam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "history_cnn = cnn.fit(X_train[:3000], y_train[:3000],\n",
    "      batch_size=32,\n",
    "      epochs=3,\n",
    "      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение данных в реальном времени\n",
    "Большие модели требуют для обучения большого количества данных. Кроме того, иногда в задачах бывает крайне мало данных. В случае с изображениями в `keras` есть отличный инструмент для увеличения (раздувания) обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False,\n",
    "                             featurewise_center=True)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще, функция `ImageDataGenerator` - очень мощный инструмент препроцессинга. Например, он позволяет стандартизировать (нормализовать на среднее 0 и стандартное отклонение на 1) изображения попиксельно (достаточно укзать `featurewise_std_normalization=True` как её аргумент), а так же уменьшать избыточность матрицы изображений (`zca_whitening=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False,\n",
    "                             featurewise_std_normalization=True,\n",
    "                             zca_whitening=True)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда возникает необходимость сохранить \"раздутую\" выборку. Для этого можно воспользоваться следующими командами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Создаем папку, куда сохраним изображения\n",
    "os.makedirs('images')\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png'):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общие советы по дополнению выборки:\n",
    "* Смотрите на исходную выборку\n",
    "* Смотрите на раздутую выборку\n",
    "* Пробуйте разные трансформации, иногда неожиданные сочетания могут дать заметный прирост"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (Fine Tuning) или зачем изобретать велосипед, когда можно встать на плечи гигантов?\n",
    "\n",
    "Идея: взять уже натренированную на большом датасете большую нейросеть и переделать её под себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Несмотря на то, что мы не будем тренировать бОльшую часть модели, код ниже будет работать долго на любом cpu\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Стандартизированный размер картинок для загружаемой сети. Это важный параметр! \n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'cats_dogs/train'\n",
    "validation_data_dir = 'cats_dogs/validation'\n",
    "nb_train_samples = 10000\n",
    "nb_validation_samples = 1000\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "# Загружаем модель VCG16\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# Создаем надстройку над ней для наших целей\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(64, activation='selu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Сшиваем модели в одну\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Замораживаем (не тренируем) первые слои сети, чтобы тренировать лишь оставшиеся быстрее\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Здесь неплохо указать небольшой (на порядок или два меньше стандартного) шаг алгоритма оптимизации для более аккуратного поиска.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-2, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Раздуваем данные\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Дообучаем модель\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)\n",
    "\n",
    "# ``` Важно, изображения должны быть организованы следующим образом:\n",
    "# data/\n",
    "#     train/\n",
    "#         dogs/\n",
    "#             dog001.jpg\n",
    "#             dog002.jpg\n",
    "#             ...\n",
    "#         cats/\n",
    "#             cat001.jpg\n",
    "#             cat002.jpg\n",
    "#             ...\n",
    "#     validation/\n",
    "#         dogs/\n",
    "#             dog001.jpg\n",
    "#             dog002.jpg\n",
    "#             ...\n",
    "#         cats/\n",
    "#             cat001.jpg\n",
    "#             cat002.jpg\n",
    "#             ...\n",
    "# ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "0. [2] Поиграться с кодом, доступном в семинаре, подергать за ручки, подобавлять слои, поизменять dropout rate, batch_size, тип активации, количество фильтров свертки и т.д. посмотреть результат\n",
    "0. [2] Загрузите доступный в `keras` датасет `cifar10`, постройте сверточную нейросеть, которая дает 70 % точности на тренировочной выборке. 80%? 90%?\n",
    "0. [4] Построить на `mnist` сверточную нейросеть, в которой будет расти размер сверточных фильтров, но уменьшаться их число. Натренировать такую сеть. Нарисовать полученные натренированные фильтры, посмотреть, какие паттерны они распознают в цифрах.\n",
    "0. [5] Попробуйте с помощью `keras` и нейросетей решить задачу регрессии (или классификации), не связанную с обработкой изображений: например, загрузите датасет `boston_housing`, [доступный](https://keras.io/datasets/) в `keras`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные материалы\n",
    "Хочется отметить, что на русском языке материалов пока несравненно меньше, чем на английском:\n",
    "* [Официальная документация](https://keras.io/) - библиотека отлично документирована\n",
    "* [Keras в конкретных примерах](https://github.com/tmheo/keras_exercise) - 25 отличных jupyter notebooks\n",
    "* [Упражнения и примеры в Keras и TensorFlow](https://github.com/leriomaggio/deep-learning-keras-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stylish cell, better to compile at the beginning\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n",
    "\n",
    "\n",
    "# from IPython.html.services.config import ConfigManager\n",
    "# from IPython.utils.path import locate_profile\n",
    "# cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "# cm.update('livereveal', {\n",
    "#               'fontsize': 4,\n",
    "#               'theme': 'simple_cyr',\n",
    "#               'transition': 'zoom',\n",
    "#               'start_slideshow_at': 'selected',\n",
    "#               'height': '724',\n",
    "#               'scroll': True,\n",
    "#               'slideNumber': True\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Табличный Q-learning\n",
    "\n",
    "Сопоставим каждой паре $(s,a)$ (состояние, действие) значение полезности - Q(s,a)\n",
    "\n",
    "В простейшем случае, когда s~1,10,100; a~1,10 лучше просто выучить, что нужно делать, т.е. задать таблицу:\n",
    "\n",
    "| a\\s | $s_1$ | $s_2$  | $s_3$ | ... |\n",
    "|-----|-----|------|-----|-----|\n",
    "| $a_1$ | 1   | 0    | 8   | ... |\n",
    "| $a_2$ | 5   | 4    | 3   | ... |\n",
    "| $a_3$ | 8   | -10 | 99  | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import random\n",
    "import pandas\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Политика\n",
    "* Жадная (greedy)\n",
    "* $\\epsilon-greedy$\n",
    "\n",
    "![deepmind](https://habrastorage.org/files/e21/72b/1ec/e2172b1ec1e146c381cb17eb6e4edb05.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "        \n",
    "        # Отвечает за баланс исследование-использование (exploration vs expluatation)\n",
    "        # С вероятностью epsilon случайное действие, 1-epsilon - argmax(Q)\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # Если максимум Q достигается для двух и более действий - берем любое\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "        \n",
    "        action = self.actions[i]\n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        new_q = reward + self.gamma*maxqnew\n",
    "\n",
    "        old_q = self.q.get((state, action), None)\n",
    "        if old_q is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = old_q + self.alpha * (new_q - old_q)\n",
    "\n",
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "    return numpy.digitize(x=[value], bins=bins)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Q-learning:\n",
    "\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha \\cdot (r(s, a) + \\max Q(s') - Q(s,a)) $$\n",
    "\n",
    "$$ Q(s,a) \\to r(s, a) + \\max Q(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v0\n",
    "\n",
    "![cartpole](http://www.cs.colostate.edu/~bwedward/assets/img/cartpole.png)\n",
    "## Environment\n",
    "\n",
    "### Observation\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Actions\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±20.9°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-03 14:23:11,241] Making new env: CartPole-v0\n",
      "[2017-10-03 14:23:11,249] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/cartpole-experiment-1')\n",
      "[2017-10-03 14:23:11,253] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "env = gym.wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "    # video_callable=lambda count: count % 10 == 0)\n",
    "\n",
    "goal_average_steps = 195\n",
    "max_number_of_steps = 200\n",
    "last_time_steps = numpy.ndarray(0)\n",
    "n_bins = 8\n",
    "n_bins_angle = 10\n",
    "\n",
    "number_of_features = env.observation_space.shape[0]\n",
    "last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "# Number of states is huge so in order to simplify the situation\n",
    "# we discretize the space to: 10 ** number_of_features\n",
    "cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "# The Q-learn algorithm\n",
    "qlearn = QLearn(actions=range(env.action_space.n),\n",
    "                alpha=0.5, gamma=0.90, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Старт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "Overall score: 177.83\n",
      "Best 100 score: 200.00\n"
     ]
    }
   ],
   "source": [
    "for i_episode in xrange(3000):\n",
    "    observation = env.reset()\n",
    "\n",
    "    cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "    state = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                     to_bin(pole_angle, pole_angle_bins),\n",
    "                     to_bin(cart_velocity, cart_velocity_bins),\n",
    "                     to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "    for t in xrange(max_number_of_steps):\n",
    "        # env.render()\n",
    "\n",
    "        # Pick an action based on the current state\n",
    "        action = qlearn.chooseAction(state)\n",
    "        # Execute the action and get feedback\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Digitize the observation to get a state\n",
    "        cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "        nextState = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                         to_bin(pole_angle, pole_angle_bins),\n",
    "                         to_bin(cart_velocity, cart_velocity_bins),\n",
    "                         to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "\n",
    "        # # If out of bounds\n",
    "        # if (cart_position > 2.4 or cart_position < -2.4):\n",
    "        #     reward = -200\n",
    "        #     qlearn.learn(state, action, reward, nextState)\n",
    "        #     print(\"Out of bounds, reseting\")\n",
    "        #     break\n",
    "\n",
    "        if not(done):\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "            state = nextState\n",
    "        else:\n",
    "            # Q-learn stuff\n",
    "            reward = -200\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "            last_time_steps = numpy.append(last_time_steps, [int(t + 1)])\n",
    "            break\n",
    "    print i_episode\n",
    "l = last_time_steps.tolist()\n",
    "l.sort()\n",
    "print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "print(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "env.close()\n",
    "# gym.upload('/tmp/cartpole-experiment-1', algorithm_id='vmayoral simple Q-learning', api_key='your-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-03 14:25:15,311] [CartPole-v0] Uploading 3000 episodes of training data\n",
      "[2017-10-03 14:25:15,412] An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 196))\n",
      "\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "You must provide an OpenAI Gym API key.\n\n(HINT: Set your API key using \"gym.scoreboard.api_key = ..\" or \"export OPENAI_GYM_API_KEY=...\"). You can find your API key in the OpenAI Gym web interface: https://gym.openai.com/settings/profile.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-52ae0781997f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Потешим самолюбие и опубликуем результат на https://gym.openai.com/envs/CartPole-v0/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/cartpole-experiment-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/api.pyc\u001b[0m in \u001b[0;36mupload\u001b[0;34m(training_dir, algorithm_id, writeup, tags, benchmark_id, api_key, ignore_open_monitors, skip_videos)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mignore_open_monitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_open_monitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mskip_videos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_videos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/api.pyc\u001b[0m in \u001b[0;36m_upload\u001b[0;34m(training_dir, algorithm_id, writeup, benchmark_run_id, api_key, ignore_open_monitors, skip_videos)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Still have an open monitor on {}. You must run 'env.close()' before uploading.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0menv_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_episode_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_videos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_videos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0menv_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'env_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mtraining_episode_batch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_video_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/api.pyc\u001b[0m in \u001b[0;36mupload_training_data\u001b[0;34m(training_dir, api_key, skip_videos)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# Do the relevant uploads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mtraining_episode_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_training_episode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_reset_timestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mtraining_episode_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/api.pyc\u001b[0m in \u001b[0;36mupload_training_episode_batch\u001b[0;34m(data_sources, episode_lengths, episode_rewards, episode_types, initial_reset_timestamps, timestamps, api_key, env_id)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_training_episode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_reset_timestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%s] Uploading %d episodes of training data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mfile_upload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileUpload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpurpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'episode_batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     file_upload.put({\n\u001b[1;32m    213\u001b[0m         \u001b[0;34m'data_sources'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/client/resource.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, **params)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         response, api_key = requestor.request(\n\u001b[0;32m--> 358\u001b[0;31m             'post', url, params=params)\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_gym_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/client/api_requestor.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         rbody, rcode, rheaders, my_api_key = self.request_raw(\n\u001b[0;32m---> 40\u001b[0;31m             method.lower(), url, params, headers)\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/scoreboard/client/api_requestor.pyc\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers)\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise error.AuthenticationError(\"\"\"You must provide an OpenAI Gym API key.\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m (HINT: Set your API key using \"gym.scoreboard.api_key = ..\" or \"export OPENAI_GYM_API_KEY=...\"). You can find your API key in the OpenAI Gym web interface: https://gym.openai.com/settings/profile.\"\"\")\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: You must provide an OpenAI Gym API key.\n\n(HINT: Set your API key using \"gym.scoreboard.api_key = ..\" or \"export OPENAI_GYM_API_KEY=...\"). You can find your API key in the OpenAI Gym web interface: https://gym.openai.com/settings/profile."
     ]
    }
   ],
   "source": [
    "#Потешим самолюбие и опубликуем результат на https://gym.openai.com/envs/CartPole-v0/\n",
    "gym.scoreboard.api_key = \\\n",
    "gym.upload('/tmp/cartpole-experiment-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-10-03 12:47:01,080] Making new env: CartPole-v0\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:157: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"lecun_uniform\", input_shape=(4,))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:166: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"lecun_uniform\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:171: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, kernel_initializer=\"lecun_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               1500      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 92,402\n",
      "Trainable params: 92,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 300)               1500      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 92,402\n",
      "Trainable params: 92,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1\n",
      "decrease reward\n",
      "Episode  0  finished after 24 timesteps\n",
      "0.995\n",
      "decrease reward\n",
      "Episode  1  finished after 33 timesteps\n",
      "0.990025\n",
      "decrease reward\n",
      "Episode  2  finished after 9 timesteps\n",
      "0.985074875\n",
      "decrease reward\n",
      "Episode  3  finished after 17 timesteps\n",
      "0.980149500625\n",
      "decrease reward\n",
      "Episode  4  finished after 13 timesteps\n",
      "0.975248753122\n",
      "decrease reward\n",
      "Episode  5  finished after 13 timesteps\n",
      "0.970372509356\n",
      "decrease reward\n",
      "Episode  6  finished after 22 timesteps\n",
      "0.965520646809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  7  finished after 12 timesteps\n",
      "0.960693043575\n",
      "decrease reward\n",
      "Episode  8  finished after 14 timesteps\n",
      "0.955889578358\n",
      "decrease reward\n",
      "Episode  9  finished after 12 timesteps\n",
      "0.951110130466\n",
      "decrease reward\n",
      "Episode  10  finished after 10 timesteps\n",
      "0.946354579813\n",
      "decrease reward\n",
      "Episode  11  finished after 20 timesteps\n",
      "0.941622806914\n",
      "decrease reward\n",
      "Episode  12  finished after 16 timesteps\n",
      "0.93691469288\n",
      "decrease reward\n",
      "Episode  13  finished after 18 timesteps\n",
      "0.932230119415\n",
      "decrease reward\n",
      "Episode  14  finished after 13 timesteps\n",
      "0.927568968818\n",
      "decrease reward\n",
      "Episode  15  finished after 15 timesteps\n",
      "0.922931123974\n",
      "decrease reward\n",
      "Episode  16  finished after 28 timesteps\n",
      "0.918316468354\n",
      "decrease reward\n",
      "Episode  17  finished after 42 timesteps\n",
      "0.913724886013\n",
      "decrease reward\n",
      "Episode  18  finished after 15 timesteps\n",
      "0.909156261583\n",
      "decrease reward\n",
      "Episode  19  finished after 13 timesteps\n",
      "0.904610480275\n",
      "decrease reward\n",
      "Episode  20  finished after 15 timesteps\n",
      "0.900087427873\n",
      "decrease reward\n",
      "Episode  21  finished after 13 timesteps\n",
      "0.895586990734\n",
      "decrease reward\n",
      "Episode  22  finished after 27 timesteps\n",
      "0.89110905578\n",
      "decrease reward\n",
      "Episode  23  finished after 22 timesteps\n",
      "0.886653510501\n",
      "decrease reward\n",
      "Episode  24  finished after 16 timesteps\n",
      "0.882220242949\n",
      "decrease reward\n",
      "Episode  25  finished after 15 timesteps\n",
      "0.877809141734\n",
      "decrease reward\n",
      "Episode  26  finished after 28 timesteps\n",
      "0.873420096025\n",
      "decrease reward\n",
      "Episode  27  finished after 14 timesteps\n",
      "0.869052995545\n",
      "decrease reward\n",
      "Episode  28  finished after 20 timesteps\n",
      "0.864707730568\n",
      "decrease reward\n",
      "Episode  29  finished after 38 timesteps\n",
      "0.860384191915\n",
      "decrease reward\n",
      "Episode  30  finished after 17 timesteps\n",
      "0.856082270955\n",
      "decrease reward\n",
      "Episode  31  finished after 17 timesteps\n",
      "0.8518018596\n",
      "decrease reward\n",
      "Episode  32  finished after 16 timesteps\n",
      "0.847542850302\n",
      "decrease reward\n",
      "Episode  33  finished after 31 timesteps\n",
      "0.843305136051\n",
      "decrease reward\n",
      "Episode  34  finished after 21 timesteps\n",
      "0.839088610371\n",
      "decrease reward\n",
      "Episode  35  finished after 63 timesteps\n",
      "0.834893167319\n",
      "decrease reward\n",
      "Episode  36  finished after 34 timesteps\n",
      "0.830718701482\n",
      "decrease reward\n",
      "Episode  37  finished after 32 timesteps\n",
      "0.826565107975\n",
      "decrease reward\n",
      "Episode  38  finished after 12 timesteps\n",
      "0.822432282435\n",
      "decrease reward\n",
      "Episode  39  finished after 80 timesteps\n",
      "0.818320121023\n",
      "decrease reward\n",
      "Episode  40  finished after 57 timesteps\n",
      "0.814228520418\n",
      "decrease reward\n",
      "Episode  41  finished after 16 timesteps\n",
      "0.810157377815\n",
      "decrease reward\n",
      "Episode  42  finished after 18 timesteps\n",
      "0.806106590926\n",
      "decrease reward\n",
      "Episode  43  finished after 78 timesteps\n",
      "0.802076057972\n",
      "decrease reward\n",
      "Episode  44  finished after 63 timesteps\n",
      "0.798065677682\n",
      "decrease reward\n",
      "Episode  45  finished after 13 timesteps\n",
      "0.794075349293\n",
      "decrease reward\n",
      "Episode  46  finished after 61 timesteps\n",
      "0.790104972547\n",
      "decrease reward\n",
      "Episode  47  finished after 37 timesteps\n",
      "0.786154447684\n",
      "decrease reward\n",
      "Episode  48  finished after 57 timesteps\n",
      "0.782223675446\n",
      "decrease reward\n",
      "Episode  49  finished after 30 timesteps\n",
      "0.778312557069\n",
      "decrease reward\n",
      "Episode  50  finished after 25 timesteps\n",
      "0.774420994283\n",
      "decrease reward\n",
      "Episode  51  finished after 28 timesteps\n",
      "0.770548889312\n",
      "decrease reward\n",
      "Episode  52  finished after 36 timesteps\n",
      "0.766696144865\n",
      "decrease reward\n",
      "Episode  53  finished after 12 timesteps\n",
      "0.762862664141\n",
      "decrease reward\n",
      "Episode  54  finished after 73 timesteps\n",
      "0.75904835082\n",
      "decrease reward\n",
      "Episode  55  finished after 12 timesteps\n",
      "0.755253109066\n",
      "decrease reward\n",
      "Episode  56  finished after 25 timesteps\n",
      "0.751476843521\n",
      "decrease reward\n",
      "Episode  57  finished after 44 timesteps\n",
      "0.747719459303\n",
      "decrease reward\n",
      "Episode  58  finished after 27 timesteps\n",
      "0.743980862007\n",
      "decrease reward\n",
      "Episode  59  finished after 141 timesteps\n",
      "0.740260957697\n",
      "decrease reward\n",
      "Episode  60  finished after 17 timesteps\n",
      "0.736559652908\n",
      "decrease reward\n",
      "Episode  61  finished after 36 timesteps\n",
      "0.732876854644\n",
      "decrease reward\n",
      "Episode  62  finished after 91 timesteps\n",
      "0.72921247037\n",
      "decrease reward\n",
      "Episode  63  finished after 22 timesteps\n",
      "0.725566408019\n",
      "decrease reward\n",
      "Episode  64  finished after 65 timesteps\n",
      "0.721938575979\n",
      "decrease reward\n",
      "Episode  65  finished after 31 timesteps\n",
      "0.718328883099\n",
      "decrease reward\n",
      "Episode  66  finished after 54 timesteps\n",
      "0.714737238683\n",
      "decrease reward\n",
      "Episode  67  finished after 78 timesteps\n",
      "0.71116355249\n",
      "decrease reward\n",
      "Episode  68  finished after 35 timesteps\n",
      "0.707607734727\n",
      "decrease reward\n",
      "Episode  69  finished after 20 timesteps\n",
      "0.704069696054\n",
      "decrease reward\n",
      "Episode  70  finished after 13 timesteps\n",
      "0.700549347573\n",
      "decrease reward\n",
      "Episode  71  finished after 19 timesteps\n",
      "0.697046600835\n",
      "decrease reward\n",
      "Episode  72  finished after 10 timesteps\n",
      "0.693561367831\n",
      "decrease reward\n",
      "Episode  73  finished after 34 timesteps\n",
      "0.690093560992\n",
      "decrease reward\n",
      "Episode  74  finished after 103 timesteps\n",
      "0.686643093187\n",
      "decrease reward\n",
      "Episode  75  finished after 46 timesteps\n",
      "0.683209877721\n",
      "decrease reward\n",
      "Episode  76  finished after 115 timesteps\n",
      "0.679793828333\n",
      "decrease reward\n",
      "Episode  77  finished after 92 timesteps\n",
      "0.676394859191\n",
      "decrease reward\n",
      "Episode  78  finished after 41 timesteps\n",
      "0.673012884895\n",
      "decrease reward\n",
      "Episode  79  finished after 19 timesteps\n",
      "0.669647820471\n",
      "decrease reward\n",
      "Episode  80  finished after 137 timesteps\n",
      "0.666299581368\n",
      "decrease reward\n",
      "Episode  81  finished after 17 timesteps\n",
      "0.662968083461\n",
      "decrease reward\n",
      "Episode  82  finished after 84 timesteps\n",
      "0.659653243044\n",
      "decrease reward\n",
      "Episode  83  finished after 50 timesteps\n",
      "0.656354976829\n",
      "decrease reward\n",
      "Episode  84  finished after 101 timesteps\n",
      "0.653073201945\n",
      "decrease reward\n",
      "Episode  85  finished after 170 timesteps\n",
      "0.649807835935\n",
      "decrease reward\n",
      "Episode  86  finished after 33 timesteps\n",
      "0.646558796755\n",
      "decrease reward\n",
      "Episode  87  finished after 31 timesteps\n",
      "0.643326002772\n",
      "decrease reward\n",
      "Episode  88  finished after 57 timesteps\n",
      "0.640109372758\n",
      "decrease reward\n",
      "Episode  89  finished after 12 timesteps\n",
      "0.636908825894\n",
      "decrease reward\n",
      "Episode  90  finished after 42 timesteps\n",
      "0.633724281764\n",
      "decrease reward\n",
      "Episode  91  finished after 119 timesteps\n",
      "0.630555660356\n",
      "decrease reward\n",
      "Episode  92  finished after 156 timesteps\n",
      "0.627402882054\n",
      "decrease reward\n",
      "Episode  93  finished after 69 timesteps\n",
      "0.624265867644\n",
      "decrease reward\n",
      "Episode  94  finished after 47 timesteps\n",
      "0.621144538305\n",
      "reached the end! :D\n",
      "Episode  95  finished after 200 timesteps\n",
      "0.618038815614\n",
      "decrease reward\n",
      "Episode  96  finished after 63 timesteps\n",
      "0.614948621536\n",
      "decrease reward\n",
      "Episode  97  finished after 76 timesteps\n",
      "0.611873878428\n",
      "decrease reward\n",
      "Episode  98  finished after 125 timesteps\n",
      "0.608814509036\n",
      "decrease reward\n",
      "Episode  99  finished after 100 timesteps  last 100 average:  42\n",
      "0.605770436491\n",
      "decrease reward\n",
      "Episode  100  finished after 37 timesteps  last 100 average:  42\n",
      "0.602741584308\n",
      "decrease reward\n",
      "Episode  101  finished after 139 timesteps  last 100 average:  43\n",
      "0.599727876387\n",
      "decrease reward\n",
      "Episode  102  finished after 72 timesteps  last 100 average:  44\n",
      "0.596729237005\n",
      "decrease reward\n",
      "Episode  103  finished after 25 timesteps  last 100 average:  44\n",
      "0.59374559082\n",
      "decrease reward\n",
      "Episode  104  finished after 46 timesteps  last 100 average:  44\n",
      "0.590776862866\n",
      "decrease reward\n",
      "Episode  105  finished after 63 timesteps  last 100 average:  45\n",
      "0.587822978551\n",
      "decrease reward\n",
      "Episode  106  finished after 16 timesteps  last 100 average:  45\n",
      "0.584883863659\n",
      "decrease reward\n",
      "Episode  107  finished after 24 timesteps  last 100 average:  45\n",
      "0.58195944434\n",
      "decrease reward\n",
      "Episode  108  finished after 124 timesteps  last 100 average:  46\n",
      "0.579049647119\n",
      "decrease reward\n",
      "Episode  109  finished after 24 timesteps  last 100 average:  46\n",
      "0.576154398883\n",
      "decrease reward\n",
      "Episode  110  finished after 139 timesteps  last 100 average:  48\n",
      "0.573273626889\n",
      "decrease reward\n",
      "Episode  111  finished after 36 timesteps  last 100 average:  48\n",
      "0.570407258754\n",
      "decrease reward\n",
      "Episode  112  finished after 42 timesteps  last 100 average:  48\n",
      "0.56755522246\n",
      "decrease reward\n",
      "Episode  113  finished after 129 timesteps  last 100 average:  49\n",
      "0.564717446348\n",
      "decrease reward\n",
      "Episode  114  finished after 15 timesteps  last 100 average:  49\n",
      "0.561893859116\n",
      "decrease reward\n",
      "Episode  115  finished after 19 timesteps  last 100 average:  49\n",
      "0.559084389821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  116  finished after 20 timesteps  last 100 average:  49\n",
      "0.556288967872\n",
      "reached the end! :D\n",
      "Episode  117  finished after 200 timesteps  last 100 average:  51\n",
      "0.553507523032\n",
      "decrease reward\n",
      "Episode  118  finished after 51 timesteps  last 100 average:  51\n",
      "0.550739985417\n",
      "decrease reward\n",
      "Episode  119  finished after 177 timesteps  last 100 average:  53\n",
      "0.54798628549\n",
      "decrease reward\n",
      "Episode  120  finished after 35 timesteps  last 100 average:  53\n",
      "0.545246354063\n",
      "decrease reward\n",
      "Episode  121  finished after 29 timesteps  last 100 average:  53\n",
      "0.542520122292\n",
      "decrease reward\n",
      "Episode  122  finished after 17 timesteps  last 100 average:  53\n",
      "0.539807521681\n",
      "decrease reward\n",
      "Episode  123  finished after 137 timesteps  last 100 average:  54\n",
      "0.537108484072\n",
      "decrease reward\n",
      "Episode  124  finished after 135 timesteps  last 100 average:  55\n",
      "0.534422941652\n",
      "decrease reward\n",
      "Episode  125  finished after 153 timesteps  last 100 average:  57\n",
      "0.531750826944\n",
      "decrease reward\n",
      "Episode  126  finished after 127 timesteps  last 100 average:  58\n",
      "0.529092072809\n",
      "decrease reward\n",
      "Episode  127  finished after 116 timesteps  last 100 average:  59\n",
      "0.526446612445\n",
      "decrease reward\n",
      "Episode  128  finished after 161 timesteps  last 100 average:  60\n",
      "0.523814379383\n",
      "decrease reward\n",
      "Episode  129  finished after 37 timesteps  last 100 average:  60\n",
      "0.521195307486\n",
      "decrease reward\n",
      "Episode  130  finished after 32 timesteps  last 100 average:  60\n",
      "0.518589330948\n",
      "decrease reward\n",
      "Episode  131  finished after 10 timesteps  last 100 average:  60\n",
      "0.515996384294\n",
      "decrease reward\n",
      "Episode  132  finished after 101 timesteps  last 100 average:  61\n",
      "0.513416402372\n",
      "decrease reward\n",
      "Episode  133  finished after 150 timesteps  last 100 average:  62\n",
      "0.51084932036\n",
      "decrease reward\n",
      "Episode  134  finished after 144 timesteps  last 100 average:  63\n",
      "0.508295073759\n",
      "decrease reward\n",
      "Episode  135  finished after 145 timesteps  last 100 average:  64\n",
      "0.50575359839\n",
      "decrease reward\n",
      "Episode  136  finished after 150 timesteps  last 100 average:  65\n",
      "0.503224830398\n",
      "decrease reward\n",
      "Episode  137  finished after 134 timesteps  last 100 average:  66\n",
      "0.500708706246\n",
      "reached the end! :D\n",
      "Episode  138  finished after 200 timesteps  last 100 average:  68\n",
      "0.498205162715\n",
      "decrease reward\n",
      "Episode  139  finished after 127 timesteps  last 100 average:  69\n",
      "0.495714136901\n",
      "decrease reward\n",
      "Episode  140  finished after 143 timesteps  last 100 average:  70\n",
      "0.493235566217\n",
      "decrease reward\n",
      "Episode  141  finished after 180 timesteps  last 100 average:  71\n",
      "0.490769388385\n",
      "reached the end! :D\n",
      "Episode  142  finished after 200 timesteps  last 100 average:  73\n",
      "0.488315541444\n",
      "decrease reward\n",
      "Episode  143  finished after 178 timesteps  last 100 average:  74\n",
      "0.485873963736\n",
      "decrease reward\n",
      "Episode  144  finished after 36 timesteps  last 100 average:  74\n",
      "0.483444593918\n",
      "decrease reward\n",
      "Episode  145  finished after 31 timesteps  last 100 average:  74\n",
      "0.481027370948\n",
      "decrease reward\n",
      "Episode  146  finished after 32 timesteps  last 100 average:  74\n",
      "0.478622234093\n",
      "decrease reward\n",
      "Episode  147  finished after 146 timesteps  last 100 average:  75\n",
      "0.476229122923\n",
      "decrease reward\n",
      "Episode  148  finished after 132 timesteps  last 100 average:  75\n",
      "0.473847977308\n",
      "decrease reward\n",
      "Episode  149  finished after 36 timesteps  last 100 average:  76\n",
      "0.471478737422\n",
      "decrease reward\n",
      "Episode  150  finished after 155 timesteps  last 100 average:  77\n",
      "0.469121343735\n",
      "decrease reward\n",
      "Episode  151  finished after 55 timesteps  last 100 average:  77\n",
      "0.466775737016\n",
      "decrease reward\n",
      "Episode  152  finished after 183 timesteps  last 100 average:  79\n",
      "0.464441858331\n",
      "decrease reward\n",
      "Episode  153  finished after 116 timesteps  last 100 average:  80\n",
      "0.462119649039\n",
      "decrease reward\n",
      "Episode  154  finished after 145 timesteps  last 100 average:  80\n",
      "0.459809050794\n",
      "decrease reward\n",
      "Episode  155  finished after 26 timesteps  last 100 average:  80\n",
      "0.45751000554\n",
      "decrease reward\n",
      "Episode  156  finished after 67 timesteps  last 100 average:  81\n",
      "0.455222455512\n",
      "decrease reward\n",
      "Episode  157  finished after 122 timesteps  last 100 average:  82\n",
      "0.452946343235\n",
      "decrease reward\n",
      "Episode  158  finished after 131 timesteps  last 100 average:  83\n",
      "0.450681611519\n",
      "updating target network\n",
      "decrease reward\n",
      "Episode  159  finished after 175 timesteps  last 100 average:  83\n",
      "0.448428203461\n",
      "decrease reward\n",
      "Episode  160  finished after 121 timesteps  last 100 average:  84\n",
      "0.446186062444\n",
      "decrease reward\n",
      "Episode  161  finished after 170 timesteps  last 100 average:  85\n",
      "0.443955132131\n",
      "decrease reward\n",
      "Episode  162  finished after 105 timesteps  last 100 average:  86\n",
      "0.441735356471\n",
      "decrease reward\n",
      "Episode  163  finished after 151 timesteps  last 100 average:  87\n",
      "0.439526679688\n",
      "decrease reward\n",
      "Episode  164  finished after 173 timesteps  last 100 average:  88\n",
      "0.43732904629\n",
      "decrease reward\n",
      "Episode  165  finished after 159 timesteps  last 100 average:  89\n",
      "0.435142401059\n",
      "reached the end! :D\n",
      "Episode  166  finished after 200 timesteps  last 100 average:  91\n",
      "0.432966689053\n",
      "decrease reward\n",
      "Episode  167  finished after 156 timesteps  last 100 average:  91\n",
      "0.430801855608\n",
      "decrease reward\n",
      "Episode  168  finished after 25 timesteps  last 100 average:  91\n",
      "0.42864784633\n",
      "decrease reward\n",
      "Episode  169  finished after 170 timesteps  last 100 average:  93\n",
      "0.426504607098\n",
      "decrease reward\n",
      "Episode  170  finished after 126 timesteps  last 100 average:  94\n",
      "0.424372084063\n",
      "decrease reward\n",
      "Episode  171  finished after 111 timesteps  last 100 average:  95\n",
      "0.422250223642\n",
      "decrease reward\n",
      "Episode  172  finished after 121 timesteps  last 100 average:  96\n",
      "0.420138972524\n",
      "decrease reward\n",
      "Episode  173  finished after 177 timesteps  last 100 average:  97\n",
      "0.418038277662\n",
      "decrease reward\n",
      "Episode  174  finished after 145 timesteps  last 100 average:  98\n",
      "0.415948086273\n",
      "decrease reward\n",
      "Episode  175  finished after 137 timesteps  last 100 average:  99\n",
      "0.413868345842\n",
      "decrease reward\n",
      "Episode  176  finished after 196 timesteps  last 100 average:  100\n",
      "0.411799004113\n",
      "decrease reward\n",
      "Episode  177  finished after 131 timesteps  last 100 average:  100\n",
      "0.409740009092\n",
      "decrease reward\n",
      "Episode  178  finished after 142 timesteps  last 100 average:  101\n",
      "0.407691309047\n",
      "decrease reward\n",
      "Episode  179  finished after 150 timesteps  last 100 average:  102\n",
      "0.405652852502\n",
      "decrease reward\n",
      "Episode  180  finished after 23 timesteps  last 100 average:  101\n",
      "0.403624588239\n",
      "decrease reward\n",
      "Episode  181  finished after 150 timesteps  last 100 average:  103\n",
      "0.401606465298\n",
      "reached the end! :D\n",
      "Episode  182  finished after 200 timesteps  last 100 average:  104\n",
      "0.399598432971\n",
      "reached the end! :D\n",
      "Episode  183  finished after 200 timesteps  last 100 average:  105\n",
      "0.397600440806\n",
      "decrease reward\n",
      "Episode  184  finished after 104 timesteps  last 100 average:  105\n",
      "0.395612438602\n",
      "decrease reward\n",
      "Episode  185  finished after 58 timesteps  last 100 average:  104\n",
      "0.393634376409\n",
      "reached the end! :D\n",
      "Episode  186  finished after 200 timesteps  last 100 average:  106\n",
      "0.391666204527\n",
      "decrease reward\n",
      "Episode  187  finished after 141 timesteps  last 100 average:  107\n",
      "0.389707873505\n",
      "reached the end! :D\n",
      "Episode  188  finished after 200 timesteps  last 100 average:  108\n",
      "0.387759334137\n",
      "decrease reward\n",
      "Episode  189  finished after 175 timesteps  last 100 average:  110\n",
      "0.385820537467\n",
      "decrease reward\n",
      "Episode  190  finished after 166 timesteps  last 100 average:  111\n",
      "0.383891434779\n",
      "decrease reward\n",
      "Episode  191  finished after 135 timesteps  last 100 average:  111\n",
      "0.381971977605\n",
      "decrease reward\n",
      "Episode  192  finished after 74 timesteps  last 100 average:  110\n",
      "0.380062117717\n",
      "decrease reward\n",
      "Episode  193  finished after 160 timesteps  last 100 average:  111\n",
      "0.378161807129\n",
      "decrease reward\n",
      "Episode  194  finished after 171 timesteps  last 100 average:  113\n",
      "0.376270998093\n",
      "decrease reward\n",
      "Episode  195  finished after 123 timesteps  last 100 average:  112\n",
      "0.374389643103\n",
      "decrease reward\n",
      "Episode  196  finished after 129 timesteps  last 100 average:  113\n",
      "0.372517694887\n",
      "decrease reward\n",
      "Episode  197  finished after 119 timesteps  last 100 average:  113\n",
      "0.370655106413\n",
      "decrease reward\n",
      "Episode  198  finished after 154 timesteps  last 100 average:  113\n",
      "0.368801830881\n",
      "decrease reward\n",
      "Episode  199  finished after 123 timesteps  last 100 average:  113\n",
      "0.366957821726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  200  finished after 150 timesteps  last 100 average:  115\n",
      "0.365123032618\n",
      "decrease reward\n",
      "Episode  201  finished after 184 timesteps  last 100 average:  115\n",
      "0.363297417454\n",
      "reached the end! :D\n",
      "Episode  202  finished after 200 timesteps  last 100 average:  116\n",
      "0.361480930367\n",
      "decrease reward\n",
      "Episode  203  finished after 141 timesteps  last 100 average:  118\n",
      "0.359673525715\n",
      "reached the end! :D\n",
      "Episode  204  finished after 200 timesteps  last 100 average:  119\n",
      "0.357875158087\n",
      "decrease reward\n",
      "Episode  205  finished after 193 timesteps  last 100 average:  120\n",
      "0.356085782296\n",
      "decrease reward\n",
      "Episode  206  finished after 122 timesteps  last 100 average:  121\n",
      "0.354305353385\n",
      "decrease reward\n",
      "Episode  207  finished after 125 timesteps  last 100 average:  122\n",
      "0.352533826618\n",
      "decrease reward\n",
      "Episode  208  finished after 160 timesteps  last 100 average:  123\n",
      "0.350771157485\n",
      "decrease reward\n",
      "Episode  209  finished after 67 timesteps  last 100 average:  123\n",
      "0.349017301697\n",
      "decrease reward\n",
      "Episode  210  finished after 179 timesteps  last 100 average:  124\n",
      "0.347272215189\n",
      "decrease reward\n",
      "Episode  211  finished after 39 timesteps  last 100 average:  124\n",
      "0.345535854113\n",
      "decrease reward\n",
      "Episode  212  finished after 133 timesteps  last 100 average:  125\n",
      "0.343808174842\n",
      "decrease reward\n",
      "Episode  213  finished after 191 timesteps  last 100 average:  125\n",
      "0.342089133968\n",
      "decrease reward\n",
      "Episode  214  finished after 144 timesteps  last 100 average:  126\n",
      "0.340378688298\n",
      "decrease reward\n",
      "Episode  215  finished after 159 timesteps  last 100 average:  128\n",
      "0.338676794857\n",
      "decrease reward\n",
      "Episode  216  finished after 194 timesteps  last 100 average:  130\n",
      "0.336983410883\n",
      "decrease reward\n",
      "Episode  217  finished after 126 timesteps  last 100 average:  129\n",
      "0.335298493828\n",
      "decrease reward\n",
      "Episode  218  finished after 194 timesteps  last 100 average:  130\n",
      "0.333622001359\n",
      "decrease reward\n",
      "Episode  219  finished after 143 timesteps  last 100 average:  130\n",
      "0.331953891352\n",
      "decrease reward\n",
      "Episode  220  finished after 168 timesteps  last 100 average:  131\n",
      "0.330294121895\n",
      "decrease reward\n",
      "Episode  221  finished after 145 timesteps  last 100 average:  132\n",
      "0.328642651286\n",
      "reached the end! :D\n",
      "Episode  222  finished after 200 timesteps  last 100 average:  134\n",
      "0.32699943803\n",
      "reached the end! :D\n",
      "Episode  223  finished after 200 timesteps  last 100 average:  135\n",
      "0.325364440839\n",
      "decrease reward\n",
      "Episode  224  finished after 181 timesteps  last 100 average:  135\n",
      "0.323737618635\n",
      "decrease reward\n",
      "Episode  225  finished after 96 timesteps  last 100 average:  135\n",
      "0.322118930542\n",
      "reached the end! :D\n",
      "Episode  226  finished after 200 timesteps  last 100 average:  136\n",
      "0.320508335889\n",
      "updating target network\n",
      "decrease reward\n",
      "Episode  227  finished after 165 timesteps  last 100 average:  136\n",
      "0.31890579421\n",
      "decrease reward\n",
      "Episode  228  finished after 145 timesteps  last 100 average:  136\n",
      "0.317311265239\n",
      "decrease reward\n",
      "Episode  229  finished after 166 timesteps  last 100 average:  137\n",
      "0.315724708913\n",
      "decrease reward\n",
      "Episode  230  finished after 55 timesteps  last 100 average:  137\n",
      "0.314146085368\n",
      "reached the end! :D\n",
      "Episode  231  finished after 200 timesteps  last 100 average:  139\n",
      "0.312575354941\n",
      "reached the end! :D\n",
      "Episode  232  finished after 200 timesteps  last 100 average:  140\n",
      "0.311012478167\n",
      "decrease reward\n",
      "Episode  233  finished after 33 timesteps  last 100 average:  139\n",
      "0.309457415776\n",
      "decrease reward\n",
      "Episode  234  finished after 161 timesteps  last 100 average:  139\n",
      "0.307910128697\n",
      "reached the end! :D\n",
      "Episode  235  finished after 200 timesteps  last 100 average:  140\n",
      "0.306370578053\n",
      "decrease reward\n",
      "Episode  236  finished after 39 timesteps  last 100 average:  139\n",
      "0.304838725163\n",
      "decrease reward\n",
      "Episode  237  finished after 178 timesteps  last 100 average:  139\n",
      "0.303314531537\n",
      "decrease reward\n",
      "Episode  238  finished after 150 timesteps  last 100 average:  139\n",
      "0.30179795888\n",
      "decrease reward\n",
      "Episode  239  finished after 164 timesteps  last 100 average:  139\n",
      "0.300288969085\n",
      "reached the end! :D\n",
      "Episode  240  finished after 200 timesteps  last 100 average:  140\n",
      "0.29878752424\n",
      "decrease reward\n",
      "Episode  241  finished after 161 timesteps  last 100 average:  139\n",
      "0.297293586619\n",
      "decrease reward\n",
      "Episode  242  finished after 141 timesteps  last 100 average:  139\n",
      "0.295807118685\n",
      "reached the end! :D\n",
      "Episode  243  finished after 200 timesteps  last 100 average:  139\n",
      "0.294328083092\n",
      "reached the end! :D\n",
      "Episode  244  finished after 200 timesteps  last 100 average:  141\n",
      "0.292856442677\n",
      "decrease reward\n",
      "Episode  245  finished after 142 timesteps  last 100 average:  142\n",
      "0.291392160463\n",
      "decrease reward\n",
      "Episode  246  finished after 182 timesteps  last 100 average:  143\n",
      "0.289935199661\n",
      "decrease reward\n",
      "Episode  247  finished after 174 timesteps  last 100 average:  144\n",
      "0.288485523663\n",
      "decrease reward\n",
      "Episode  248  finished after 163 timesteps  last 100 average:  144\n",
      "0.287043096044\n",
      "decrease reward\n",
      "Episode  249  finished after 156 timesteps  last 100 average:  145\n",
      "0.285607880564\n",
      "decrease reward\n",
      "Episode  250  finished after 177 timesteps  last 100 average:  145\n",
      "0.284179841161\n",
      "decrease reward\n",
      "Episode  251  finished after 192 timesteps  last 100 average:  147\n",
      "0.282758941955\n",
      "decrease reward\n",
      "Episode  252  finished after 144 timesteps  last 100 average:  146\n",
      "0.281345147246\n",
      "decrease reward\n",
      "Episode  253  finished after 138 timesteps  last 100 average:  146\n",
      "0.279938421509\n",
      "decrease reward\n",
      "Episode  254  finished after 197 timesteps  last 100 average:  147\n",
      "0.278538729402\n",
      "decrease reward\n",
      "Episode  255  finished after 161 timesteps  last 100 average:  148\n",
      "0.277146035755\n",
      "decrease reward\n",
      "Episode  256  finished after 162 timesteps  last 100 average:  149\n",
      "0.275760305576\n",
      "decrease reward\n",
      "Episode  257  finished after 161 timesteps  last 100 average:  150\n",
      "0.274381504048\n",
      "decrease reward\n",
      "Episode  258  finished after 189 timesteps  last 100 average:  150\n",
      "0.273009596528\n",
      "decrease reward\n",
      "Episode  259  finished after 176 timesteps  last 100 average:  150\n",
      "0.271644548545\n",
      "reached the end! :D\n",
      "Episode  260  finished after 200 timesteps  last 100 average:  151\n",
      "0.270286325803\n",
      "decrease reward\n",
      "Episode  261  finished after 189 timesteps  last 100 average:  151\n",
      "0.268934894174\n",
      "reached the end! :D\n",
      "Episode  262  finished after 200 timesteps  last 100 average:  152\n",
      "0.267590219703\n",
      "decrease reward\n",
      "Episode  263  finished after 172 timesteps  last 100 average:  152\n",
      "0.266252268604\n",
      "decrease reward\n",
      "Episode  264  finished after 183 timesteps  last 100 average:  153\n",
      "0.264921007261\n",
      "reached the end! :D\n",
      "Episode  265  finished after 200 timesteps  last 100 average:  153\n",
      "0.263596402225\n",
      "decrease reward\n",
      "Episode  266  finished after 150 timesteps  last 100 average:  152\n",
      "0.262278420214\n",
      "reached the end! :D\n",
      "Episode  267  finished after 200 timesteps  last 100 average:  153\n",
      "0.260967028113\n",
      "reached the end! :D\n",
      "Episode  268  finished after 200 timesteps  last 100 average:  155\n",
      "0.259662192972\n",
      "reached the end! :D\n",
      "Episode  269  finished after 200 timesteps  last 100 average:  155\n",
      "0.258363882007\n",
      "reached the end! :D\n",
      "Episode  270  finished after 200 timesteps  last 100 average:  156\n",
      "0.257072062597\n",
      "reached the end! :D\n",
      "Episode  271  finished after 200 timesteps  last 100 average:  157\n",
      "0.255786702284\n",
      "decrease reward\n",
      "Episode  272  finished after 153 timesteps  last 100 average:  157\n",
      "0.254507768773\n",
      "decrease reward\n",
      "Episode  273  finished after 161 timesteps  last 100 average:  157\n",
      "0.253235229929\n",
      "decrease reward\n",
      "Episode  274  finished after 166 timesteps  last 100 average:  157\n",
      "0.251969053779\n",
      "decrease reward\n",
      "Episode  275  finished after 169 timesteps  last 100 average:  157\n",
      "0.25070920851\n",
      "reached the end! :D\n",
      "Episode  276  finished after 200 timesteps  last 100 average:  157\n",
      "0.249455662468\n",
      "reached the end! :D\n",
      "Episode  277  finished after 200 timesteps  last 100 average:  158\n",
      "0.248208384156\n",
      "decrease reward\n",
      "Episode  278  finished after 163 timesteps  last 100 average:  158\n",
      "0.246967342235\n",
      "decrease reward\n",
      "Episode  279  finished after 188 timesteps  last 100 average:  159\n",
      "0.245732505524\n",
      "reached the end! :D\n",
      "Episode  280  finished after 200 timesteps  last 100 average:  160\n",
      "0.244503842996\n",
      "decrease reward\n",
      "Episode  281  finished after 171 timesteps  last 100 average:  161\n",
      "0.243281323781\n",
      "reached the end! :D\n",
      "Episode  282  finished after 200 timesteps  last 100 average:  161\n",
      "0.242064917162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  283  finished after 165 timesteps  last 100 average:  160\n",
      "0.240854592576\n",
      "decrease reward\n",
      "Episode  284  finished after 165 timesteps  last 100 average:  161\n",
      "0.239650319613\n",
      "reached the end! :D\n",
      "Episode  285  finished after 200 timesteps  last 100 average:  162\n",
      "0.238452068015\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  286  finished after 200 timesteps  last 100 average:  162\n",
      "0.237259807675\n",
      "decrease reward\n",
      "Episode  287  finished after 166 timesteps  last 100 average:  162\n",
      "0.236073508637\n",
      "decrease reward\n",
      "Episode  288  finished after 163 timesteps  last 100 average:  162\n",
      "0.234893141094\n",
      "decrease reward\n",
      "Episode  289  finished after 150 timesteps  last 100 average:  162\n",
      "0.233718675388\n",
      "decrease reward\n",
      "Episode  290  finished after 181 timesteps  last 100 average:  162\n",
      "0.232550082011\n",
      "decrease reward\n",
      "Episode  291  finished after 184 timesteps  last 100 average:  162\n",
      "0.231387331601\n",
      "reached the end! :D\n",
      "Episode  292  finished after 200 timesteps  last 100 average:  164\n",
      "0.230230394943\n",
      "decrease reward\n",
      "Episode  293  finished after 180 timesteps  last 100 average:  164\n",
      "0.229079242968\n",
      "decrease reward\n",
      "Episode  294  finished after 171 timesteps  last 100 average:  164\n",
      "0.227933846754\n",
      "decrease reward\n",
      "Episode  295  finished after 185 timesteps  last 100 average:  165\n",
      "0.22679417752\n",
      "decrease reward\n",
      "Episode  296  finished after 188 timesteps  last 100 average:  165\n",
      "0.225660206632\n",
      "decrease reward\n",
      "Episode  297  finished after 181 timesteps  last 100 average:  166\n",
      "0.224531905599\n",
      "reached the end! :D\n",
      "Episode  298  finished after 200 timesteps  last 100 average:  166\n",
      "0.223409246071\n",
      "reached the end! :D\n",
      "Episode  299  finished after 200 timesteps  last 100 average:  167\n",
      "0.222292199841\n",
      "decrease reward\n",
      "Episode  300  finished after 187 timesteps  last 100 average:  167\n",
      "0.221180738842\n",
      "reached the end! :D\n",
      "Episode  301  finished after 200 timesteps  last 100 average:  168\n",
      "0.220074835147\n",
      "decrease reward\n",
      "Episode  302  finished after 150 timesteps  last 100 average:  167\n",
      "0.218974460972\n",
      "reached the end! :D\n",
      "Episode  303  finished after 200 timesteps  last 100 average:  168\n",
      "0.217879588667\n",
      "decrease reward\n",
      "Episode  304  finished after 198 timesteps  last 100 average:  168\n",
      "0.216790190723\n",
      "decrease reward\n",
      "Episode  305  finished after 183 timesteps  last 100 average:  168\n",
      "0.21570623977\n",
      "reached the end! :D\n",
      "Episode  306  finished after 200 timesteps  last 100 average:  168\n",
      "0.214627708571\n",
      "decrease reward\n",
      "Episode  307  finished after 175 timesteps  last 100 average:  169\n",
      "0.213554570028\n",
      "decrease reward\n",
      "Episode  308  finished after 196 timesteps  last 100 average:  169\n",
      "0.212486797178\n",
      "reached the end! :D\n",
      "Episode  309  finished after 200 timesteps  last 100 average:  170\n",
      "0.211424363192\n",
      "decrease reward\n",
      "Episode  310  finished after 103 timesteps  last 100 average:  170\n",
      "0.210367241376\n",
      "reached the end! :D\n",
      "Episode  311  finished after 200 timesteps  last 100 average:  171\n",
      "0.209315405169\n",
      "reached the end! :D\n",
      "Episode  312  finished after 200 timesteps  last 100 average:  172\n",
      "0.208268828143\n",
      "reached the end! :D\n",
      "Episode  313  finished after 200 timesteps  last 100 average:  172\n",
      "0.207227484003\n",
      "decrease reward\n",
      "Episode  314  finished after 75 timesteps  last 100 average:  171\n",
      "0.206191346583\n",
      "reached the end! :D\n",
      "Episode  315  finished after 200 timesteps  last 100 average:  172\n",
      "0.20516038985\n",
      "decrease reward\n",
      "Episode  316  finished after 160 timesteps  last 100 average:  171\n",
      "0.2041345879\n",
      "decrease reward\n",
      "Episode  317  finished after 185 timesteps  last 100 average:  172\n",
      "0.203113914961\n",
      "decrease reward\n",
      "Episode  318  finished after 151 timesteps  last 100 average:  172\n",
      "0.202098345386\n",
      "decrease reward\n",
      "Episode  319  finished after 174 timesteps  last 100 average:  172\n",
      "0.201087853659\n",
      "reached the end! :D\n",
      "Episode  320  finished after 200 timesteps  last 100 average:  172\n",
      "0.200082414391\n",
      "decrease reward\n",
      "Episode  321  finished after 171 timesteps  last 100 average:  173\n",
      "0.199082002319\n",
      "decrease reward\n",
      "Episode  322  finished after 188 timesteps  last 100 average:  172\n",
      "0.198086592307\n",
      "decrease reward\n",
      "Episode  323  finished after 191 timesteps  last 100 average:  172\n",
      "0.197096159346\n",
      "reached the end! :D\n",
      "Episode  324  finished after 200 timesteps  last 100 average:  172\n",
      "0.196110678549\n",
      "decrease reward\n",
      "Episode  325  finished after 176 timesteps  last 100 average:  173\n",
      "0.195130125156\n",
      "reached the end! :D\n",
      "Episode  326  finished after 200 timesteps  last 100 average:  173\n",
      "0.194154474531\n",
      "decrease reward\n",
      "Episode  327  finished after 195 timesteps  last 100 average:  174\n",
      "0.193183702158\n",
      "reached the end! :D\n",
      "Episode  328  finished after 200 timesteps  last 100 average:  174\n",
      "0.192217783647\n",
      "decrease reward\n",
      "Episode  329  finished after 163 timesteps  last 100 average:  174\n",
      "0.191256694729\n",
      "reached the end! :D\n",
      "Episode  330  finished after 200 timesteps  last 100 average:  176\n",
      "0.190300411255\n",
      "decrease reward\n",
      "Episode  331  finished after 194 timesteps  last 100 average:  176\n",
      "0.189348909199\n",
      "reached the end! :D\n",
      "Episode  332  finished after 200 timesteps  last 100 average:  176\n",
      "0.188402164653\n",
      "reached the end! :D\n",
      "Episode  333  finished after 200 timesteps  last 100 average:  177\n",
      "0.18746015383\n",
      "decrease reward\n",
      "Episode  334  finished after 166 timesteps  last 100 average:  177\n",
      "0.186522853061\n",
      "decrease reward\n",
      "Episode  335  finished after 197 timesteps  last 100 average:  177\n",
      "0.185590238795\n",
      "reached the end! :D\n",
      "Episode  336  finished after 200 timesteps  last 100 average:  179\n",
      "0.184662287601\n",
      "reached the end! :D\n",
      "Episode  337  finished after 200 timesteps  last 100 average:  179\n",
      "0.183738976163\n",
      "decrease reward\n",
      "Episode  338  finished after 181 timesteps  last 100 average:  179\n",
      "0.182820281282\n",
      "reached the end! :D\n",
      "Episode  339  finished after 200 timesteps  last 100 average:  180\n",
      "0.181906179876\n",
      "reached the end! :D\n",
      "Episode  340  finished after 200 timesteps  last 100 average:  180\n",
      "0.180996648977\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  341  finished after 200 timesteps  last 100 average:  180\n",
      "0.180091665732\n",
      "decrease reward\n",
      "Episode  342  finished after 179 timesteps  last 100 average:  180\n",
      "0.179191207403\n",
      "reached the end! :D\n",
      "Episode  343  finished after 200 timesteps  last 100 average:  180\n",
      "0.178295251366\n",
      "reached the end! :D\n",
      "Episode  344  finished after 200 timesteps  last 100 average:  180\n",
      "0.177403775109\n",
      "decrease reward\n",
      "Episode  345  finished after 165 timesteps  last 100 average:  181\n",
      "0.176516756234\n",
      "decrease reward\n",
      "Episode  346  finished after 195 timesteps  last 100 average:  181\n",
      "0.175634172453\n",
      "decrease reward\n",
      "Episode  347  finished after 188 timesteps  last 100 average:  181\n",
      "0.17475600159\n",
      "decrease reward\n",
      "Episode  348  finished after 151 timesteps  last 100 average:  181\n",
      "0.173882221582\n",
      "reached the end! :D\n",
      "Episode  349  finished after 200 timesteps  last 100 average:  181\n",
      "0.173012810474\n",
      "reached the end! :D\n",
      "Episode  350  finished after 200 timesteps  last 100 average:  182\n",
      "0.172147746422\n",
      "decrease reward\n",
      "Episode  351  finished after 174 timesteps  last 100 average:  181\n",
      "0.17128700769\n",
      "decrease reward\n",
      "Episode  352  finished after 182 timesteps  last 100 average:  182\n",
      "0.170430572652\n",
      "reached the end! :D\n",
      "Episode  353  finished after 200 timesteps  last 100 average:  182\n",
      "0.169578419788\n",
      "reached the end! :D\n",
      "Episode  354  finished after 200 timesteps  last 100 average:  182\n",
      "0.168730527689\n",
      "decrease reward\n",
      "Episode  355  finished after 162 timesteps  last 100 average:  182\n",
      "0.167886875051\n",
      "decrease reward\n",
      "Episode  356  finished after 167 timesteps  last 100 average:  182\n",
      "0.167047440676\n",
      "reached the end! :D\n",
      "Episode  357  finished after 200 timesteps  last 100 average:  183\n",
      "0.166212203472\n",
      "decrease reward\n",
      "Episode  358  finished after 187 timesteps  last 100 average:  183\n",
      "0.165381142455\n",
      "reached the end! :D\n",
      "Episode  359  finished after 200 timesteps  last 100 average:  183\n",
      "0.164554236743\n",
      "decrease reward\n",
      "Episode  360  finished after 191 timesteps  last 100 average:  183\n",
      "0.163731465559\n",
      "decrease reward\n",
      "Episode  361  finished after 175 timesteps  last 100 average:  183\n",
      "0.162912808231\n",
      "reached the end! :D\n",
      "Episode  362  finished after 200 timesteps  last 100 average:  183\n",
      "0.16209824419\n",
      "decrease reward\n",
      "Episode  363  finished after 186 timesteps  last 100 average:  183\n",
      "0.161287752969\n",
      "decrease reward\n",
      "Episode  364  finished after 178 timesteps  last 100 average:  183\n",
      "0.160481314204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  365  finished after 174 timesteps  last 100 average:  183\n",
      "0.159678907633\n",
      "decrease reward\n",
      "Episode  366  finished after 177 timesteps  last 100 average:  183\n",
      "0.158880513095\n",
      "decrease reward\n",
      "Episode  367  finished after 196 timesteps  last 100 average:  183\n",
      "0.158086110529\n",
      "decrease reward\n",
      "Episode  368  finished after 188 timesteps  last 100 average:  183\n",
      "0.157295679977\n",
      "reached the end! :D\n",
      "Episode  369  finished after 200 timesteps  last 100 average:  183\n",
      "0.156509201577\n",
      "reached the end! :D\n",
      "Episode  370  finished after 200 timesteps  last 100 average:  183\n",
      "0.155726655569\n",
      "decrease reward\n",
      "Episode  371  finished after 195 timesteps  last 100 average:  183\n",
      "0.154948022291\n",
      "reached the end! :D\n",
      "Episode  372  finished after 200 timesteps  last 100 average:  183\n",
      "0.15417328218\n",
      "reached the end! :D\n",
      "Episode  373  finished after 200 timesteps  last 100 average:  184\n",
      "0.153402415769\n",
      "decrease reward\n",
      "Episode  374  finished after 185 timesteps  last 100 average:  184\n",
      "0.15263540369\n",
      "decrease reward\n",
      "Episode  375  finished after 199 timesteps  last 100 average:  184\n",
      "0.151872226672\n",
      "decrease reward\n",
      "Episode  376  finished after 177 timesteps  last 100 average:  184\n",
      "0.151112865538\n",
      "reached the end! :D\n",
      "Episode  377  finished after 200 timesteps  last 100 average:  184\n",
      "0.150357301211\n",
      "decrease reward\n",
      "Episode  378  finished after 170 timesteps  last 100 average:  184\n",
      "0.149605514704\n",
      "decrease reward\n",
      "Episode  379  finished after 164 timesteps  last 100 average:  184\n",
      "0.148857487131\n",
      "reached the end! :D\n",
      "Episode  380  finished after 200 timesteps  last 100 average:  184\n",
      "0.148113199695\n",
      "decrease reward\n",
      "Episode  381  finished after 188 timesteps  last 100 average:  184\n",
      "0.147372633697\n",
      "decrease reward\n",
      "Episode  382  finished after 176 timesteps  last 100 average:  184\n",
      "0.146635770528\n",
      "decrease reward\n",
      "Episode  383  finished after 193 timesteps  last 100 average:  184\n",
      "0.145902591676\n",
      "decrease reward\n",
      "Episode  384  finished after 178 timesteps  last 100 average:  184\n",
      "0.145173078717\n",
      "decrease reward\n",
      "Episode  385  finished after 192 timesteps  last 100 average:  184\n",
      "0.144447213324\n",
      "decrease reward\n",
      "Episode  386  finished after 157 timesteps  last 100 average:  183\n",
      "0.143724977257\n",
      "reached the end! :D\n",
      "Episode  387  finished after 200 timesteps  last 100 average:  184\n",
      "0.143006352371\n",
      "decrease reward\n",
      "Episode  388  finished after 166 timesteps  last 100 average:  184\n",
      "0.142291320609\n",
      "decrease reward\n",
      "Episode  389  finished after 182 timesteps  last 100 average:  184\n",
      "0.141579864006\n",
      "reached the end! :D\n",
      "Episode  390  finished after 200 timesteps  last 100 average:  184\n",
      "0.140871964686\n",
      "reached the end! :D\n",
      "Episode  391  finished after 200 timesteps  last 100 average:  185\n",
      "0.140167604862\n",
      "decrease reward\n",
      "Episode  392  finished after 187 timesteps  last 100 average:  184\n",
      "0.139466766838\n",
      "decrease reward\n",
      "Episode  393  finished after 193 timesteps  last 100 average:  185\n",
      "0.138769433004\n",
      "decrease reward\n",
      "Episode  394  finished after 188 timesteps  last 100 average:  185\n",
      "0.138075585839\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  395  finished after 200 timesteps  last 100 average:  185\n",
      "0.13738520791\n",
      "decrease reward\n",
      "Episode  396  finished after 168 timesteps  last 100 average:  185\n",
      "0.13669828187\n",
      "decrease reward\n",
      "Episode  397  finished after 192 timesteps  last 100 average:  185\n",
      "0.136014790461\n",
      "decrease reward\n",
      "Episode  398  finished after 170 timesteps  last 100 average:  184\n",
      "0.135334716509\n",
      "decrease reward\n",
      "Episode  399  finished after 178 timesteps  last 100 average:  184\n",
      "0.134658042926\n",
      "decrease reward\n",
      "Episode  400  finished after 179 timesteps  last 100 average:  184\n",
      "0.133984752711\n",
      "decrease reward\n",
      "Episode  401  finished after 185 timesteps  last 100 average:  184\n",
      "0.133314828948\n",
      "decrease reward\n",
      "Episode  402  finished after 185 timesteps  last 100 average:  184\n",
      "0.132648254803\n",
      "decrease reward\n",
      "Episode  403  finished after 191 timesteps  last 100 average:  184\n",
      "0.131985013529\n",
      "decrease reward\n",
      "Episode  404  finished after 180 timesteps  last 100 average:  184\n",
      "0.131325088461\n",
      "decrease reward\n",
      "Episode  405  finished after 197 timesteps  last 100 average:  184\n",
      "0.130668463019\n",
      "decrease reward\n",
      "Episode  406  finished after 185 timesteps  last 100 average:  184\n",
      "0.130015120704\n",
      "decrease reward\n",
      "Episode  407  finished after 156 timesteps  last 100 average:  184\n",
      "0.129365045101\n",
      "decrease reward\n",
      "Episode  408  finished after 169 timesteps  last 100 average:  184\n",
      "0.128718219875\n",
      "decrease reward\n",
      "Episode  409  finished after 173 timesteps  last 100 average:  183\n",
      "0.128074628776\n",
      "decrease reward\n",
      "Episode  410  finished after 190 timesteps  last 100 average:  184\n",
      "0.127434255632\n",
      "decrease reward\n",
      "Episode  411  finished after 172 timesteps  last 100 average:  184\n",
      "0.126797084354\n",
      "reached the end! :D\n",
      "Episode  412  finished after 200 timesteps  last 100 average:  184\n",
      "0.126163098932\n",
      "decrease reward\n",
      "Episode  413  finished after 191 timesteps  last 100 average:  184\n",
      "0.125532283437\n",
      "decrease reward\n",
      "Episode  414  finished after 192 timesteps  last 100 average:  185\n",
      "0.12490462202\n",
      "decrease reward\n",
      "Episode  415  finished after 189 timesteps  last 100 average:  185\n",
      "0.12428009891\n",
      "decrease reward\n",
      "Episode  416  finished after 175 timesteps  last 100 average:  185\n",
      "0.123658698415\n",
      "decrease reward\n",
      "Episode  417  finished after 154 timesteps  last 100 average:  185\n",
      "0.123040404923\n",
      "decrease reward\n",
      "Episode  418  finished after 196 timesteps  last 100 average:  185\n",
      "0.122425202899\n",
      "decrease reward\n",
      "Episode  419  finished after 187 timesteps  last 100 average:  185\n",
      "0.121813076884\n",
      "decrease reward\n",
      "Episode  420  finished after 172 timesteps  last 100 average:  185\n",
      "0.1212040115\n",
      "decrease reward\n",
      "Episode  421  finished after 171 timesteps  last 100 average:  185\n",
      "0.120597991442\n",
      "reached the end! :D\n",
      "Episode  422  finished after 200 timesteps  last 100 average:  185\n",
      "0.119995001485\n",
      "decrease reward\n",
      "Episode  423  finished after 186 timesteps  last 100 average:  185\n",
      "0.119395026478\n",
      "decrease reward\n",
      "Episode  424  finished after 186 timesteps  last 100 average:  185\n",
      "0.118798051345\n",
      "reached the end! :D\n",
      "Episode  425  finished after 200 timesteps  last 100 average:  185\n",
      "0.118204061088\n",
      "decrease reward\n",
      "Episode  426  finished after 174 timesteps  last 100 average:  185\n",
      "0.117613040783\n",
      "decrease reward\n",
      "Episode  427  finished after 164 timesteps  last 100 average:  185\n",
      "0.117024975579\n",
      "decrease reward\n",
      "Episode  428  finished after 170 timesteps  last 100 average:  184\n",
      "0.116439850701\n",
      "decrease reward\n",
      "Episode  429  finished after 179 timesteps  last 100 average:  184\n",
      "0.115857651448\n",
      "decrease reward\n",
      "Episode  430  finished after 181 timesteps  last 100 average:  184\n",
      "0.11527836319\n",
      "reached the end! :D\n",
      "Episode  431  finished after 200 timesteps  last 100 average:  184\n",
      "0.114701971375\n",
      "decrease reward\n",
      "Episode  432  finished after 25 timesteps  last 100 average:  183\n",
      "0.114128461518\n",
      "decrease reward\n",
      "Episode  433  finished after 172 timesteps  last 100 average:  182\n",
      "0.11355781921\n",
      "decrease reward\n",
      "Episode  434  finished after 182 timesteps  last 100 average:  182\n",
      "0.112990030114\n",
      "decrease reward\n",
      "Episode  435  finished after 181 timesteps  last 100 average:  182\n",
      "0.112425079963\n",
      "decrease reward\n",
      "Episode  436  finished after 186 timesteps  last 100 average:  182\n",
      "0.111862954564\n",
      "decrease reward\n",
      "Episode  437  finished after 181 timesteps  last 100 average:  182\n",
      "0.111303639791\n",
      "decrease reward\n",
      "Episode  438  finished after 177 timesteps  last 100 average:  182\n",
      "0.110747121592\n",
      "decrease reward\n",
      "Episode  439  finished after 175 timesteps  last 100 average:  182\n",
      "0.110193385984\n",
      "reached the end! :D\n",
      "Episode  440  finished after 200 timesteps  last 100 average:  182\n",
      "0.109642419054\n",
      "decrease reward\n",
      "Episode  441  finished after 175 timesteps  last 100 average:  181\n",
      "0.109094206959\n",
      "decrease reward\n",
      "Episode  442  finished after 186 timesteps  last 100 average:  182\n",
      "0.108548735924\n",
      "decrease reward\n",
      "Episode  443  finished after 184 timesteps  last 100 average:  181\n",
      "0.108005992244\n",
      "decrease reward\n",
      "Episode  444  finished after 175 timesteps  last 100 average:  181\n",
      "0.107465962283\n",
      "decrease reward\n",
      "Episode  445  finished after 175 timesteps  last 100 average:  181\n",
      "0.106928632472\n",
      "decrease reward\n",
      "Episode  446  finished after 184 timesteps  last 100 average:  181\n",
      "0.106393989309\n",
      "decrease reward\n",
      "Episode  447  finished after 194 timesteps  last 100 average:  181\n",
      "0.105862019363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  448  finished after 189 timesteps  last 100 average:  182\n",
      "0.105332709266\n",
      "decrease reward\n",
      "Episode  449  finished after 192 timesteps  last 100 average:  181\n",
      "0.10480604572\n",
      "decrease reward\n",
      "Episode  450  finished after 191 timesteps  last 100 average:  181\n",
      "0.104282015491\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  451  finished after 200 timesteps  last 100 average:  182\n",
      "0.103760605414\n",
      "reached the end! :D\n",
      "Episode  452  finished after 200 timesteps  last 100 average:  182\n",
      "0.103241802386\n",
      "reached the end! :D\n",
      "Episode  453  finished after 200 timesteps  last 100 average:  182\n",
      "0.102725593375\n",
      "decrease reward\n",
      "Episode  454  finished after 174 timesteps  last 100 average:  182\n",
      "0.102211965408\n",
      "decrease reward\n",
      "Episode  455  finished after 189 timesteps  last 100 average:  182\n",
      "0.101700905581\n",
      "reached the end! :D\n",
      "Episode  456  finished after 200 timesteps  last 100 average:  182\n",
      "0.101192401053\n",
      "decrease reward\n",
      "Episode  457  finished after 177 timesteps  last 100 average:  182\n",
      "0.100686439047\n",
      "reached the end! :D\n",
      "Episode  458  finished after 200 timesteps  last 100 average:  182\n",
      "0.100183006852\n",
      "decrease reward\n",
      "Episode  459  finished after 194 timesteps  last 100 average:  182\n",
      "0.099682091818\n",
      "reached the end! :D\n",
      "Episode  460  finished after 200 timesteps  last 100 average:  182\n",
      "0.0991836813589\n",
      "decrease reward\n",
      "Episode  461  finished after 174 timesteps  last 100 average:  182\n",
      "0.0986877629521\n",
      "reached the end! :D\n",
      "Episode  462  finished after 200 timesteps  last 100 average:  182\n",
      "0.0981943241373\n",
      "decrease reward\n",
      "Episode  463  finished after 180 timesteps  last 100 average:  182\n",
      "0.0977033525166\n",
      "decrease reward\n",
      "Episode  464  finished after 186 timesteps  last 100 average:  182\n",
      "0.0972148357541\n",
      "reached the end! :D\n",
      "Episode  465  finished after 200 timesteps  last 100 average:  182\n",
      "0.0967287615753\n",
      "decrease reward\n",
      "Episode  466  finished after 174 timesteps  last 100 average:  182\n",
      "0.0962451177674\n",
      "decrease reward\n",
      "Episode  467  finished after 189 timesteps  last 100 average:  182\n",
      "0.0957638921786\n",
      "decrease reward\n",
      "Episode  468  finished after 173 timesteps  last 100 average:  182\n",
      "0.0952850727177\n",
      "decrease reward\n",
      "Episode  469  finished after 184 timesteps  last 100 average:  182\n",
      "0.0948086473541\n",
      "reached the end! :D\n",
      "Episode  470  finished after 200 timesteps  last 100 average:  182\n",
      "0.0943346041173\n",
      "decrease reward\n",
      "Episode  471  finished after 147 timesteps  last 100 average:  181\n",
      "0.0938629310967\n",
      "decrease reward\n",
      "Episode  472  finished after 168 timesteps  last 100 average:  181\n",
      "0.0933936164413\n",
      "decrease reward\n",
      "Episode  473  finished after 172 timesteps  last 100 average:  181\n",
      "0.092926648359\n",
      "decrease reward\n",
      "Episode  474  finished after 173 timesteps  last 100 average:  181\n",
      "0.0924620151173\n",
      "reached the end! :D\n",
      "Episode  475  finished after 200 timesteps  last 100 average:  181\n",
      "0.0919997050417\n",
      "decrease reward\n",
      "Episode  476  finished after 172 timesteps  last 100 average:  181\n",
      "0.0915397065165\n",
      "reached the end! :D\n",
      "Episode  477  finished after 200 timesteps  last 100 average:  181\n",
      "0.0910820079839\n",
      "decrease reward\n",
      "Episode  478  finished after 185 timesteps  last 100 average:  181\n",
      "0.090626597944\n",
      "decrease reward\n",
      "Episode  479  finished after 166 timesteps  last 100 average:  181\n",
      "0.0901734649542\n",
      "decrease reward\n",
      "Episode  480  finished after 153 timesteps  last 100 average:  180\n",
      "0.0897225976295\n",
      "decrease reward\n",
      "Episode  481  finished after 167 timesteps  last 100 average:  180\n",
      "0.0892739846413\n",
      "decrease reward\n",
      "Episode  482  finished after 189 timesteps  last 100 average:  180\n",
      "0.0888276147181\n",
      "decrease reward\n",
      "Episode  483  finished after 183 timesteps  last 100 average:  180\n",
      "0.0883834766445\n",
      "decrease reward\n",
      "Episode  484  finished after 122 timesteps  last 100 average:  180\n",
      "0.0879415592613\n",
      "decrease reward\n",
      "Episode  485  finished after 167 timesteps  last 100 average:  179\n",
      "0.087501851465\n",
      "decrease reward\n",
      "Episode  486  finished after 156 timesteps  last 100 average:  179\n",
      "0.0870643422077\n",
      "decrease reward\n",
      "Episode  487  finished after 138 timesteps  last 100 average:  179\n",
      "0.0866290204966\n",
      "decrease reward\n",
      "Episode  488  finished after 191 timesteps  last 100 average:  179\n",
      "0.0861958753941\n",
      "reached the end! :D\n",
      "Episode  489  finished after 200 timesteps  last 100 average:  179\n",
      "0.0857648960172\n",
      "decrease reward\n",
      "Episode  490  finished after 167 timesteps  last 100 average:  179\n",
      "0.0853360715371\n",
      "decrease reward\n",
      "Episode  491  finished after 182 timesteps  last 100 average:  179\n",
      "0.0849093911794\n",
      "decrease reward\n",
      "Episode  492  finished after 158 timesteps  last 100 average:  178\n",
      "0.0844848442235\n",
      "decrease reward\n",
      "Episode  493  finished after 179 timesteps  last 100 average:  178\n",
      "0.0840624200024\n",
      "decrease reward\n",
      "Episode  494  finished after 147 timesteps  last 100 average:  178\n",
      "0.0836421079024\n",
      "reached the end! :D\n",
      "Episode  495  finished after 200 timesteps  last 100 average:  178\n",
      "0.0832238973629\n",
      "decrease reward\n",
      "Episode  496  finished after 196 timesteps  last 100 average:  178\n",
      "0.0828077778761\n",
      "decrease reward\n",
      "Episode  497  finished after 146 timesteps  last 100 average:  178\n",
      "0.0823937389867\n",
      "decrease reward\n",
      "Episode  498  finished after 123 timesteps  last 100 average:  177\n",
      "0.0819817702917\n",
      "decrease reward\n",
      "Episode  499  finished after 166 timesteps  last 100 average:  177\n",
      "0.0815718614403\n",
      "decrease reward\n",
      "Episode  500  finished after 144 timesteps  last 100 average:  177\n",
      "0.0811640021331\n",
      "decrease reward\n",
      "Episode  501  finished after 142 timesteps  last 100 average:  176\n",
      "0.0807581821224\n",
      "decrease reward\n",
      "Episode  502  finished after 149 timesteps  last 100 average:  176\n",
      "0.0803543912118\n",
      "decrease reward\n",
      "Episode  503  finished after 147 timesteps  last 100 average:  176\n",
      "0.0799526192557\n",
      "decrease reward\n",
      "Episode  504  finished after 162 timesteps  last 100 average:  175\n",
      "0.0795528561595\n",
      "decrease reward\n",
      "Episode  505  finished after 199 timesteps  last 100 average:  175\n",
      "0.0791550918787\n",
      "decrease reward\n",
      "Episode  506  finished after 136 timesteps  last 100 average:  175\n",
      "0.0787593164193\n",
      "decrease reward\n",
      "Episode  507  finished after 158 timesteps  last 100 average:  175\n",
      "0.0783655198372\n",
      "updating target network\n",
      "decrease reward\n",
      "Episode  508  finished after 161 timesteps  last 100 average:  175\n",
      "0.077973692238\n",
      "reached the end! :D\n",
      "Episode  509  finished after 200 timesteps  last 100 average:  175\n",
      "0.0775838237768\n",
      "decrease reward\n",
      "Episode  510  finished after 186 timesteps  last 100 average:  175\n",
      "0.0771959046579\n",
      "reached the end! :D\n",
      "Episode  511  finished after 200 timesteps  last 100 average:  175\n",
      "0.0768099251346\n",
      "reached the end! :D\n",
      "Episode  512  finished after 200 timesteps  last 100 average:  175\n",
      "0.076425875509\n",
      "reached the end! :D\n",
      "Episode  513  finished after 200 timesteps  last 100 average:  175\n",
      "0.0760437461314\n",
      "reached the end! :D\n",
      "Episode  514  finished after 200 timesteps  last 100 average:  175\n",
      "0.0756635274008\n",
      "reached the end! :D\n",
      "Episode  515  finished after 200 timesteps  last 100 average:  176\n",
      "0.0752852097637\n",
      "reached the end! :D\n",
      "Episode  516  finished after 200 timesteps  last 100 average:  176\n",
      "0.0749087837149\n",
      "reached the end! :D\n",
      "Episode  517  finished after 200 timesteps  last 100 average:  176\n",
      "0.0745342397964\n",
      "reached the end! :D\n",
      "Episode  518  finished after 200 timesteps  last 100 average:  176\n",
      "0.0741615685974\n",
      "reached the end! :D\n",
      "Episode  519  finished after 200 timesteps  last 100 average:  176\n",
      "0.0737907607544\n",
      "reached the end! :D\n",
      "Episode  520  finished after 200 timesteps  last 100 average:  177\n",
      "0.0734218069506\n",
      "reached the end! :D\n",
      "Episode  521  finished after 200 timesteps  last 100 average:  177\n",
      "0.0730546979159\n",
      "reached the end! :D\n",
      "Episode  522  finished after 200 timesteps  last 100 average:  177\n",
      "0.0726894244263\n",
      "decrease reward\n",
      "Episode  523  finished after 192 timesteps  last 100 average:  177\n",
      "0.0723259773041\n",
      "reached the end! :D\n",
      "Episode  524  finished after 200 timesteps  last 100 average:  177\n",
      "0.0719643474176\n",
      "reached the end! :D\n",
      "Episode  525  finished after 200 timesteps  last 100 average:  177\n",
      "0.0716045256805\n",
      "reached the end! :D\n",
      "Episode  526  finished after 200 timesteps  last 100 average:  178\n",
      "0.0712465030521\n",
      "reached the end! :D\n",
      "Episode  527  finished after 200 timesteps  last 100 average:  178\n",
      "0.0708902705369\n",
      "decrease reward\n",
      "Episode  528  finished after 184 timesteps  last 100 average:  178\n",
      "0.0705358191842\n",
      "reached the end! :D\n",
      "Episode  529  finished after 200 timesteps  last 100 average:  178\n",
      "0.0701831400883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached the end! :D\n",
      "Episode  530  finished after 200 timesteps  last 100 average:  178\n",
      "0.0698322243878\n",
      "reached the end! :D\n",
      "Episode  531  finished after 200 timesteps  last 100 average:  178\n",
      "0.0694830632659\n",
      "reached the end! :D\n",
      "Episode  532  finished after 200 timesteps  last 100 average:  180\n",
      "0.0691356479496\n",
      "reached the end! :D\n",
      "Episode  533  finished after 200 timesteps  last 100 average:  180\n",
      "0.0687899697098\n",
      "reached the end! :D\n",
      "Episode  534  finished after 200 timesteps  last 100 average:  181\n",
      "0.0684460198613\n",
      "reached the end! :D\n",
      "Episode  535  finished after 200 timesteps  last 100 average:  181\n",
      "0.068103789762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2a01cc4461f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mdeepQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnOnMiniBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mdeepQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnOnMiniBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewObservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a2a01cc4461f>\u001b[0m in \u001b[0;36mlearnOnMiniBatch\u001b[0;34m(self, miniBatchSize, useTargetNetwork)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mnewState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newState'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0mqValues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0museTargetNetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0mqValuesNewState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTargetQValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a2a01cc4461f>\u001b[0m in \u001b[0;36mgetQValues\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# predict Q values for all the actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetQValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1517\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m           \u001b[0mfeed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m\"\"\"The string name of this tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1355\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Deep Q-learning approach to the cartpole problem\n",
    "using OpenAI's gym environment.\n",
    "\n",
    "As part of the basic series on reinforcement learning @\n",
    "https://github.com/vmayoral/basic_reinforcement_learning\n",
    "\n",
    "This code implements the algorithm described at:\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, \n",
    "S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.\n",
    "\n",
    "Code based on @wingedsheep's work at https://gist.github.com/wingedsheep/4199594b02138dd427c22a540d6d6b8d\n",
    "\n",
    "        @author: Victor Mayoral Vilches <victor@erlerobotics.com>\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# import os\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "# import theano\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    This class provides an abstraction to store the [s, a, r, a'] elements of each iteration.\n",
    "    Instead of using tuples (as other implementations do), the information is stored in lists \n",
    "    that get returned as another list of dictionaries with each key corresponding to either \n",
    "    \"state\", \"action\", \"reward\", \"nextState\" or \"isFinal\".\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.currentPosition = 0\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.newStates = []\n",
    "        self.finals = []\n",
    "\n",
    "    def getMiniBatch(self, size) :\n",
    "        indices = random.sample(np.arange(len(self.states)), min(size,len(self.states)) )\n",
    "        miniBatch = []\n",
    "        for index in indices:\n",
    "            miniBatch.append({'state': self.states[index],'action': self.actions[index], 'reward': self.rewards[index], 'newState': self.newStates[index], 'isFinal': self.finals[index]})\n",
    "        return miniBatch\n",
    "\n",
    "    def getCurrentSize(self) :\n",
    "        return len(self.states)\n",
    "\n",
    "    def getMemory(self, index): \n",
    "        return {'state': self.states[index],'action': self.actions[index], 'reward': self.rewards[index], 'newState': self.newStates[index], 'isFinal': self.finals[index]}\n",
    "\n",
    "    def addMemory(self, state, action, reward, newState, isFinal) :\n",
    "        if (self.currentPosition >= self.size - 1) :\n",
    "            self.currentPosition = 0\n",
    "        if (len(self.states) > self.size) :\n",
    "            self.states[self.currentPosition] = state\n",
    "            self.actions[self.currentPosition] = action\n",
    "            self.rewards[self.currentPosition] = reward\n",
    "            self.newStates[self.currentPosition] = newState\n",
    "            self.finals[self.currentPosition] = isFinal\n",
    "        else :\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.newStates.append(newState)\n",
    "            self.finals.append(isFinal)\n",
    "        \n",
    "        self.currentPosition += 1\n",
    "\n",
    "class DeepQ:\n",
    "    \"\"\"\n",
    "    DQN abstraction.\n",
    "\n",
    "    As a quick reminder:\n",
    "        traditional Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + gamma * max(Q(s') - Q(s,a))\n",
    "        DQN:\n",
    "            target = reward(s,a) + gamma * max(Q(s')\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, outputs, memorySize, discountFactor, learningRate, learnStart):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - inputs: input size\n",
    "            - outputs: output size\n",
    "            - memorySize: size of the memory that will store each state\n",
    "            - discountFactor: the discount factor (gamma)\n",
    "            - learningRate: learning rate\n",
    "            - learnStart: steps to happen before for learning. Set to 128\n",
    "        \"\"\"\n",
    "        self.input_size = inputs\n",
    "        self.output_size = outputs\n",
    "        self.memory = Memory(memorySize)\n",
    "        self.discountFactor = discountFactor\n",
    "        self.learnStart = learnStart\n",
    "        self.learningRate = learningRate\n",
    "   \n",
    "    def initNetworks(self, hiddenLayers):\n",
    "        model = self.createModel(self.input_size, self.output_size, hiddenLayers, \"relu\", self.learningRate)\n",
    "        self.model = model\n",
    "\n",
    "        targetModel = self.createModel(self.input_size, self.output_size, hiddenLayers, \"relu\", self.learningRate)\n",
    "        self.targetModel = targetModel\n",
    "\n",
    "    def createRegularizedModel(self, inputs, outputs, hiddenLayers, activationType, learningRate):\n",
    "        bias = True\n",
    "        dropout = 0\n",
    "        regularizationFactor = 0.01\n",
    "        model = Sequential()\n",
    "        if len(hiddenLayers) == 0: \n",
    "            model.add(Dense(self.output_size, input_shape=(self.input_size,), init='lecun_uniform', bias=bias))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        else :\n",
    "            if regularizationFactor > 0:\n",
    "                model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform', W_regularizer=l2(regularizationFactor),  bias=bias))\n",
    "            else:\n",
    "                model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform', bias=bias))\n",
    "\n",
    "            if (activationType == \"LeakyReLU\") :\n",
    "                model.add(LeakyReLU(alpha=0.01))\n",
    "            else :\n",
    "                model.add(Activation(activationType))\n",
    "            \n",
    "            for index in range(1, len(hiddenLayers)):\n",
    "                layerSize = hiddenLayers[index]\n",
    "                if regularizationFactor > 0:\n",
    "                    model.add(Dense(layerSize, init='lecun_uniform', W_regularizer=l2(regularizationFactor), bias=bias))\n",
    "                else:\n",
    "                    model.add(Dense(layerSize, init='lecun_uniform', bias=bias))\n",
    "                if (activationType == \"LeakyReLU\") :\n",
    "                    model.add(LeakyReLU(alpha=0.01))\n",
    "                else :\n",
    "                    model.add(Activation(activationType))\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "            model.add(Dense(self.output_size, init='lecun_uniform', bias=bias))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def createModel(self, inputs, outputs, hiddenLayers, activationType, learningRate):\n",
    "        model = Sequential()\n",
    "        if len(hiddenLayers) == 0: \n",
    "            model.add(Dense(self.output_size, input_shape=(self.input_size,), init='lecun_uniform'))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        else :\n",
    "            model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform'))\n",
    "            if (activationType == \"LeakyReLU\") :\n",
    "                model.add(LeakyReLU(alpha=0.01))\n",
    "            else :\n",
    "                model.add(Activation(activationType))\n",
    "            \n",
    "            for index in range(1, len(hiddenLayers)):\n",
    "                # print(\"adding layer \"+str(index))\n",
    "                layerSize = hiddenLayers[index]\n",
    "                model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "                if (activationType == \"LeakyReLU\") :\n",
    "                    model.add(LeakyReLU(alpha=0.01))\n",
    "                else :\n",
    "                    model.add(Activation(activationType))\n",
    "            model.add(Dense(self.output_size, init='lecun_uniform'))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def printNetwork(self):\n",
    "        i = 0\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            print \"layer \",i,\": \",weights\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    def backupNetwork(self, model, backup):\n",
    "        weightMatrix = []\n",
    "        for layer in model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            weightMatrix.append(weights)\n",
    "        i = 0\n",
    "        for layer in backup.layers:\n",
    "            weights = weightMatrix[i]\n",
    "            layer.set_weights(weights)\n",
    "            i += 1\n",
    "\n",
    "    def updateTargetNetwork(self):\n",
    "        self.backupNetwork(self.model, self.targetModel)\n",
    "\n",
    "    # predict Q values for all the actions\n",
    "    def getQValues(self, state):\n",
    "        predicted = self.model.predict(state.reshape(1,len(state)))\n",
    "        return predicted[0]\n",
    "\n",
    "    def getTargetQValues(self, state):\n",
    "        predicted = self.targetModel.predict(state.reshape(1,len(state)))\n",
    "        return predicted[0]\n",
    "\n",
    "    def getMaxQ(self, qValues):\n",
    "        return np.max(qValues)\n",
    "\n",
    "    def getMaxIndex(self, qValues):\n",
    "        return np.argmax(qValues)\n",
    "\n",
    "    # calculate the target function\n",
    "    def calculateTarget(self, qValuesNewState, reward, isFinal):\n",
    "        \"\"\"\n",
    "        target = reward(s,a) + gamma * max(Q(s')\n",
    "        \"\"\"\n",
    "        if isFinal:\n",
    "            return reward\n",
    "        else : \n",
    "            return reward + self.discountFactor * self.getMaxQ(qValuesNewState)\n",
    "\n",
    "    # select the action with the highest Q value\n",
    "    def selectAction(self, qValues, explorationRate):\n",
    "        rand = random.random()\n",
    "        if rand < explorationRate :\n",
    "            action = np.random.randint(0, self.output_size)\n",
    "        else :\n",
    "            action = self.getMaxIndex(qValues)\n",
    "        return action\n",
    "\n",
    "    def selectActionByProbability(self, qValues, bias):\n",
    "        qValueSum = 0\n",
    "        shiftBy = 0\n",
    "        for value in qValues:\n",
    "            if value + shiftBy < 0:\n",
    "                shiftBy = - (value + shiftBy)\n",
    "        shiftBy += 1e-06\n",
    "\n",
    "        for value in qValues:\n",
    "            qValueSum += (value + shiftBy) ** bias\n",
    "\n",
    "        probabilitySum = 0\n",
    "        qValueProbabilities = []\n",
    "        for value in qValues:\n",
    "            probability = ((value + shiftBy) ** bias) / float(qValueSum)\n",
    "            qValueProbabilities.append(probability + probabilitySum)\n",
    "            probabilitySum += probability\n",
    "        qValueProbabilities[len(qValueProbabilities) - 1] = 1\n",
    "\n",
    "        rand = random.random()\n",
    "        i = 0\n",
    "        for value in qValueProbabilities:\n",
    "            if (rand <= value):\n",
    "                return i\n",
    "            i += 1\n",
    "\n",
    "    def addMemory(self, state, action, reward, newState, isFinal):\n",
    "        self.memory.addMemory(state, action, reward, newState, isFinal)\n",
    "\n",
    "    def learnOnLastState(self):\n",
    "        if self.memory.getCurrentSize() >= 1:\n",
    "            return self.memory.getMemory(self.memory.getCurrentSize() - 1)\n",
    "\n",
    "    def learnOnMiniBatch(self, miniBatchSize, useTargetNetwork=True):\n",
    "        # Do not learn until we've got self.learnStart samples        \n",
    "        if self.memory.getCurrentSize() > self.learnStart:\n",
    "            # learn in batches of 128\n",
    "            miniBatch = self.memory.getMiniBatch(miniBatchSize)\n",
    "            X_batch = np.empty((0,self.input_size), dtype = np.float64)\n",
    "            Y_batch = np.empty((0,self.output_size), dtype = np.float64)\n",
    "            for sample in miniBatch:\n",
    "                isFinal = sample['isFinal']\n",
    "                state = sample['state']\n",
    "                action = sample['action']\n",
    "                reward = sample['reward']\n",
    "                newState = sample['newState']\n",
    "\n",
    "                qValues = self.getQValues(state)\n",
    "                if useTargetNetwork:\n",
    "                    qValuesNewState = self.getTargetQValues(newState)\n",
    "                else :\n",
    "                    qValuesNewState = self.getQValues(newState)\n",
    "                targetValue = self.calculateTarget(qValuesNewState, reward, isFinal)\n",
    "\n",
    "                X_batch = np.append(X_batch, np.array([state.copy()]), axis=0)\n",
    "                Y_sample = qValues.copy()\n",
    "                Y_sample[action] = targetValue\n",
    "                Y_batch = np.append(Y_batch, np.array([Y_sample]), axis=0)\n",
    "                if isFinal:\n",
    "                    X_batch = np.append(X_batch, np.array([newState.copy()]), axis=0)\n",
    "                    Y_batch = np.append(Y_batch, np.array([[reward]*self.output_size]), axis=0)\n",
    "            self.model.fit(X_batch, Y_batch, batch_size = len(miniBatch), nb_epoch=1, verbose = 0)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "epochs = 1000\n",
    "steps = 100000\n",
    "updateTargetNetwork = 10000\n",
    "explorationRate = 1\n",
    "minibatch_size = 128\n",
    "learnStart = 128\n",
    "learningRate = 0.00025\n",
    "discountFactor = 0.99\n",
    "memorySize = 1000000\n",
    "\n",
    "last100Scores = [0] * 100\n",
    "last100ScoresIndex = 0\n",
    "last100Filled = False\n",
    "\n",
    "deepQ = DeepQ(4, 2, memorySize, discountFactor, learningRate, learnStart)\n",
    "# deepQ.initNetworks([30,30,30])\n",
    "# deepQ.initNetworks([30,30])\n",
    "deepQ.initNetworks([300,300])\n",
    "\n",
    "stepCounter = 0\n",
    "\n",
    "# number of reruns\n",
    "for epoch in xrange(epochs):\n",
    "    observation = env.reset()\n",
    "    print explorationRate\n",
    "    # number of timesteps\n",
    "    for t in xrange(steps):\n",
    "        # env.render()\n",
    "        qValues = deepQ.getQValues(observation)\n",
    "\n",
    "        action = deepQ.selectAction(qValues, explorationRate)\n",
    "\n",
    "        newObservation, reward, done, info = env.step(action)\n",
    "\n",
    "        if (t >= 199):\n",
    "            print \"reached the end! :D\"\n",
    "            done = True\n",
    "            # reward = 200            \n",
    "\n",
    "        if done and t < 199:\n",
    "            print \"decrease reward\"\n",
    "            # reward -= 200\n",
    "        deepQ.addMemory(observation, action, reward, newObservation, done)\n",
    "\n",
    "        if stepCounter >= learnStart:\n",
    "            if stepCounter <= updateTargetNetwork:\n",
    "                deepQ.learnOnMiniBatch(minibatch_size, False)\n",
    "            else :\n",
    "                deepQ.learnOnMiniBatch(minibatch_size, True)\n",
    "\n",
    "        observation = newObservation\n",
    "\n",
    "        if done:\n",
    "            last100Scores[last100ScoresIndex] = t\n",
    "            last100ScoresIndex += 1\n",
    "            if last100ScoresIndex >= 100:\n",
    "                last100Filled = True\n",
    "                last100ScoresIndex = 0\n",
    "            if not last100Filled:\n",
    "                print \"Episode \",epoch,\" finished after {} timesteps\".format(t+1)\n",
    "            else :\n",
    "                print \"Episode \",epoch,\" finished after {} timesteps\".format(t+1),\" last 100 average: \",(sum(last100Scores)/len(last100Scores))\n",
    "            break\n",
    "\n",
    "        stepCounter += 1\n",
    "        if stepCounter % updateTargetNetwork == 0:\n",
    "            deepQ.updateTargetNetwork()\n",
    "            print \"updating target network\"\n",
    "\n",
    "    explorationRate *= 0.995\n",
    "    # explorationRate -= (2.0/epochs)\n",
    "    explorationRate = max (0.05, explorationRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
