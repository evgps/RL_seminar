{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Q-learning в средах OpenAI.gym</center></h1>\n",
    "\n",
    "### Евгений Пономарев\n",
    "\n",
    "### Сколковский институт науки и технологий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеки:\n",
    "* `pip install numpy`\n",
    "* `pip install tensorflow`\n",
    "* `pip install gym`\n",
    "* `pip install keras`\n",
    "\n",
    "ffmpeg\n",
    "Документация к openai.gym:\n",
    "* https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением aka Reinforcement learning:\n",
    "![](https://keras.io/img/keras-logo-small-wb.png)\n",
    "* Библиотека для машинного обучения (прежде всего, обучения нейронных сетей, в т.ч. глубоких). \n",
    "* Представляет собой удобную обертку для мощных и хорошо оптимизированных вычислительных библиотек: TensorFlow, Theano\n",
    "* Основные принципы: \n",
    "    1. Удобство использования\n",
    "    2. Модульность\n",
    "    3. Масштабируемость\n",
    "    4. Работа с Python\n",
    "    \n",
    "Инструмент с низким порогом входа, подходящий как продвинутым исследовалетям, так и энтузиастам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras: краткий обзор\n",
    "![](https://keras.io/img/keras-logo-small-wb.png)\n",
    "* Библиотека для машинного обучения (прежде всего, обучения нейронных сетей, в т.ч. глубоких). \n",
    "* Представляет собой удобную обертку для мощных и хорошо оптимизированных вычислительных библиотек: TensorFlow, Theano\n",
    "* Основные принципы: \n",
    "    1. Удобство использования\n",
    "    2. Модульность\n",
    "    3. Масштабируемость\n",
    "    4. Работа с Python\n",
    "    \n",
    "Инструмент с низким порогом входа, подходящий как продвинутым исследовалетям, так и энтузиастам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возвращение MNIST. Сравнения\n",
    "Давайте вернемся к старой доброй задаче распознавания рукописных цифр, чтобы посмотреть сколько занимает код для такой нейросети, написанный на keras. Кроме того, здесь же мы попробуем собрать простую сверточную сеть для улучшения результата.\n",
    "\n",
    "### Классическая нейросеть с плотными (dense) слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import keras.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка данных. В keras уже есть несколько популярных датасетов, которые можно легко загрузить. Давайте загрузим MNIST\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Подготовка датасета: нормализация значений на [0,1] и перевод признаков в one-hot формат\n",
    "X_train, X_test = X_train/255, X_test/255\n",
    "y_train, y_test = keras.utils.to_categorical(y_train, 10), keras.utils.to_categorical(y_test, 10)\n",
    "input_size = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создание модели. Sequential здесь означает последовательный тип модели, в который мы добавляем слои друг за другом\n",
    "model = Sequential()\n",
    "\n",
    "# Добавляем в стек модели слой за слоем. Полносвязный, активация и т.д.\n",
    "# Важно: в первом слое Sequential модели keras необходимо указать размерность входных данных.\n",
    "model.add(Flatten(input_shape=input_size))\n",
    "model.add(Dense(units=256, input_shape=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.6187 - acc: 0.8470     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.3332 - acc: 0.9079     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2851 - acc: 0.9205     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2539 - acc: 0.9290     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s - loss: 0.2297 - acc: 0.9355     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2751ac917f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После описания архитектуры необходимо скомпилировать модель, указав минимизируемую функцию потерь, \n",
    "# оптимизатор и попросив модель выводить точность работы на тестовой выорке в процессе обучения\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Тренировка с указанием данных, числа эпох и размера подвыборки\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21461834994256496, 0.93879999999999997]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим качество работы модели на тестовых данных. Выводится loss и точночть.\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим сверточный слой и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "# Создание модели. Sequential здесь означает последовательный тип модели, в который мы добавляем слои друг за другом\n",
    "conv_model = Sequential()\n",
    "\n",
    "# Добавим явно число каналов в наш датасет - это важно для сверточных слоев. \n",
    "# т.е. делается преобразование (60000, 28, 28) -> (60000, 28, 28, 1). Это ничего не изменяет.\n",
    "X_train, X_test = X_train.reshape((60000, 28, 28, 1)), X_test.reshape((10000, 28, 28, 1))\n",
    "input_size = X_train[0].shape\n",
    "\n",
    "# Здесь мы используем сверточный слой, который тренирует 32 фильтра размером 3x3 для поиска \n",
    "# конкретных геометрических (настраиваемых в процессе обучения)паттернов на входном изображении.\n",
    "conv_model.add(Conv2D(24, (3, 3), input_shape=input_size))\n",
    "conv_model.add(Activation('selu'))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(64, activation='selu'))\n",
    "conv_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10816/30000 [=========>....................] - ETA: 34s - loss: 0.4272 - acc: 0.8696"
     ]
    }
   ],
   "source": [
    "# Поставим другой оптимизатор для разнообразия\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Тренировка с указанием данных, числа эпох и размера подвыборки\n",
    "conv_model.fit(X_train[:30000], y_train[:30000], epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проверим качество работы модели на тестовых данных. Выводится loss и точночть.\n",
    "conv_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточные слои, dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируются и обучаются несколько фильтров небольших размеров так, чтобы распознавать какие то характерные сочетания пикселей (паттерны). Ниже на картинке изображение 5x5 пикселей, фильтр имеет размеры 3x3. При этом на выходе такой операции имеем картинку такого же размера, в каждый из пикселей которого записан результат свертки (число) данного фильтра с картинкой при нахождении центра фильтра в этом пикселе. Для этого исходную картинку необходимо дополнить по краям. Обычно это делают либо нулями, либо дублируют ближайшие пиксели (padding)\n",
    "![](convol.gif)\n",
    "\n",
    "Чем глубже сверточный слой, тем более сложные паттерны он способен распознавать:\n",
    "![](features.png)\n",
    "\n",
    "Dropout - техника спасения нейросетей от переобучения, при которой в процессе тренировки случайно \"выключаются\" некоторые нейроны из моделей.\n",
    "\n",
    "Альтернативный взгляд - вместо тренировки одной большой сети проходит одновременная тренировка нескольких подсетей меньшего размера, результаты которых потом усредняются (в каком то смысле, сглаживаются).\n",
    "![](dropout.gif)\n",
    "\n",
    "Давайте попробуем посмотреть, как написать сеть, состоящую из нескольких сверточных слоев на `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D, Dropout\n",
    "\n",
    "# Все так же, создаем модель\n",
    "cnn = Sequential()\n",
    "\n",
    "# Начинаем со сверточного слоя, указывая тип активации на выходе из него и способ заполнения краев (padding)\n",
    "cnn.add(Conv2D(64, (3, 3), input_shape=input_size, activation='selu', padding='same'))\n",
    "\n",
    "# Здесь мы используем метод MaxPooling, который уменьшает размер обрабатываемого изображения, \n",
    "# выбирая из 4 пикселей 1 с максимальным значением, чтобы это быстрее считалось. (2,2) -> 1\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Слой dropout, который на каждом шаге \"выключает\" 25% случайно выбранных нейронов\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "# Еще сверточный слой\n",
    "cnn.add(Conv2D(32, (3, 3), input_shape=input_size, activation='selu', padding='same'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "# Последний слой необходим для классификации, но перед ним необходимо векторизовать данные\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = 'nadam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "history_cnn = cnn.fit(X_train[:3000], y_train[:3000],\n",
    "      batch_size=32,\n",
    "      epochs=3,\n",
    "      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение данных в реальном времени\n",
    "Большие модели требуют для обучения большого количества данных. Кроме того, иногда в задачах бывает крайне мало данных. В случае с изображениями в `keras` есть отличный инструмент для увеличения (раздувания) обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False,\n",
    "                             featurewise_center=True)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще, функция `ImageDataGenerator` - очень мощный инструмент препроцессинга. Например, он позволяет стандартизировать (нормализовать на среднее 0 и стандартное отклонение на 1) изображения попиксельно (достаточно укзать `featurewise_std_normalization=True` как её аргумент), а так же уменьшать избыточность матрицы изображений (`zca_whitening=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False,\n",
    "                             featurewise_std_normalization=True,\n",
    "                             zca_whitening=True)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда возникает необходимость сохранить \"раздутую\" выборку. Для этого можно воспользоваться следующими командами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Задаем параметры, в рамках которых выборка может дополняться. Эти параметры сильно зависят от выборки.\n",
    "# Например, в задаче распознавания рукописных цифр отражение изображения не особо релевантно, т.к. в тестовой выборке таких встретиться не может\n",
    "shift = 0.1 # максимальное значени сдвига в долях (от 0 до 1)\n",
    "angle = 15   # максимальный угол поворота \n",
    "\n",
    "# Команда создает генератор, который при вызове в режиме реального времени генерирует необходимую подвыборку нужного размера\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, \n",
    "                             height_shift_range=shift, \n",
    "                             rotation_range=angle, \n",
    "                             horizontal_flip=False, \n",
    "                             vertical_flip=False)\n",
    "\n",
    "# Подстраиваем генератор под наши данные\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Создаем папку, куда сохраним изображения\n",
    "os.makedirs('images')\n",
    "\n",
    "# Выберем случайно 9 картинок дополненной выборки и нарисуем их\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png'):\n",
    "    # создаем сетку 3х3\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # рисуем картинки\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общие советы по дополнению выборки:\n",
    "* Смотрите на исходную выборку\n",
    "* Смотрите на раздутую выборку\n",
    "* Пробуйте разные трансформации, иногда неожиданные сочетания могут дать заметный прирост"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (Fine Tuning) или зачем изобретать велосипед, когда можно встать на плечи гигантов?\n",
    "\n",
    "Идея: взять уже натренированную на большом датасете большую нейросеть и переделать её под себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Несмотря на то, что мы не будем тренировать бОльшую часть модели, код ниже будет работать долго на любом cpu\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Стандартизированный размер картинок для загружаемой сети. Это важный параметр! \n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'cats_dogs/train'\n",
    "validation_data_dir = 'cats_dogs/validation'\n",
    "nb_train_samples = 10000\n",
    "nb_validation_samples = 1000\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "# Загружаем модель VCG16\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# Создаем надстройку над ней для наших целей\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(64, activation='selu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Сшиваем модели в одну\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Замораживаем (не тренируем) первые слои сети, чтобы тренировать лишь оставшиеся быстрее\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Здесь неплохо указать небольшой (на порядок или два меньше стандартного) шаг алгоритма оптимизации для более аккуратного поиска.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-2, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Раздуваем данные\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Дообучаем модель\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)\n",
    "\n",
    "# ``` Важно, изображения должны быть организованы следующим образом:\n",
    "# data/\n",
    "#     train/\n",
    "#         dogs/\n",
    "#             dog001.jpg\n",
    "#             dog002.jpg\n",
    "#             ...\n",
    "#         cats/\n",
    "#             cat001.jpg\n",
    "#             cat002.jpg\n",
    "#             ...\n",
    "#     validation/\n",
    "#         dogs/\n",
    "#             dog001.jpg\n",
    "#             dog002.jpg\n",
    "#             ...\n",
    "#         cats/\n",
    "#             cat001.jpg\n",
    "#             cat002.jpg\n",
    "#             ...\n",
    "# ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "0. [2] Поиграться с кодом, доступном в семинаре, подергать за ручки, подобавлять слои, поизменять dropout rate, batch_size, тип активации, количество фильтров свертки и т.д. посмотреть результат\n",
    "0. [2] Загрузите доступный в `keras` датасет `cifar10`, постройте сверточную нейросеть, которая дает 70 % точности на тренировочной выборке. 80%? 90%?\n",
    "0. [4] Построить на `mnist` сверточную нейросеть, в которой будет расти размер сверточных фильтров, но уменьшаться их число. Натренировать такую сеть. Нарисовать полученные натренированные фильтры, посмотреть, какие паттерны они распознают в цифрах.\n",
    "0. [5] Попробуйте с помощью `keras` и нейросетей решить задачу регрессии (или классификации), не связанную с обработкой изображений: например, загрузите датасет `boston_housing`, [доступный](https://keras.io/datasets/) в `keras`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные материалы\n",
    "Хочется отметить, что на русском языке материалов пока несравненно меньше, чем на английском:\n",
    "* [Официальная документация](https://keras.io/) - библиотека отлично документирована\n",
    "* [Keras в конкретных примерах](https://github.com/tmheo/keras_exercise) - 25 отличных jupyter notebooks\n",
    "* [Упражнения и примеры в Keras и TensorFlow](https://github.com/leriomaggio/deep-learning-keras-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stylish cell, better to compile at the beginning\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n",
    "\n",
    "\n",
    "# from IPython.html.services.config import ConfigManager\n",
    "# from IPython.utils.path import locate_profile\n",
    "# cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "# cm.update('livereveal', {\n",
    "#               'fontsize': 4,\n",
    "#               'theme': 'simple_cyr',\n",
    "#               'transition': 'zoom',\n",
    "#               'start_slideshow_at': 'selected',\n",
    "#               'height': '724',\n",
    "#               'scroll': True,\n",
    "#               'slideNumber': True\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Табличный Q-learning\n",
    "\n",
    "Сопоставим каждой паре $(s,a)$ (состояние, действие) значение полезности - Q(s,a)\n",
    "\n",
    "В простейшем случае, когда s~1,10,100; a~1,10 лучше просто выучить, что нужно делать, т.е. задать таблицу:\n",
    "\n",
    "| a\\s | $s_1$ | $s_2$  | $s_3$ | ... |\n",
    "|-----|-----|------|-----|-----|\n",
    "| $a_1$ | 1   | 0    | 8   | ... |\n",
    "| $a_2$ | 5   | 4    | 3   | ... |\n",
    "| $a_3$ | 8   | -10 | 99  | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import random\n",
    "import pandas\n",
    "\n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values\n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = self.actions[i]\n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "    return numpy.digitize(x=[value], bins=bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-03 10:55:28,780] Making new env: CartPole-v0\n",
      "[2017-10-03 10:55:28,793] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/cartpole-experiment-1')\n",
      "[2017-10-03 10:55:28,797] Clearing 27 monitor files from previous run (because force=True was provided)\n",
      "[2017-10-03 10:55:28,807] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000000.mp4\n",
      "[2017-10-03 10:55:29,436] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000001.mp4\n",
      "[2017-10-03 10:55:29,674] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000008.mp4\n",
      "[2017-10-03 10:55:29,920] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000027.mp4\n",
      "[2017-10-03 10:55:33,369] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000064.mp4\n",
      "[2017-10-03 10:55:36,679] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000125.mp4\n",
      "[2017-10-03 10:55:40,160] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000216.mp4\n",
      "[2017-10-03 10:55:43,408] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000343.mp4\n",
      "[2017-10-03 10:55:47,350] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000512.mp4\n",
      "[2017-10-03 10:55:50,815] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video000729.mp4\n",
      "[2017-10-03 10:55:54,985] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video001000.mp4\n",
      "[2017-10-03 10:56:01,540] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.2.11868.video002000.mp4\n",
      "[2017-10-03 10:56:08,516] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/cartpole-experiment-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 179.29\n",
      "Best 100 score: 200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "env = gym.wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "    # video_callable=lambda count: count % 10 == 0)\n",
    "\n",
    "goal_average_steps = 195\n",
    "max_number_of_steps = 200\n",
    "last_time_steps = numpy.ndarray(0)\n",
    "n_bins = 8\n",
    "n_bins_angle = 10\n",
    "\n",
    "number_of_features = env.observation_space.shape[0]\n",
    "last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "# Number of states is huge so in order to simplify the situation\n",
    "# we discretize the space to: 10 ** number_of_features\n",
    "cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "# The Q-learn algorithm\n",
    "qlearn = QLearn(actions=range(env.action_space.n),\n",
    "                alpha=0.5, gamma=0.90, epsilon=0.1)\n",
    "\n",
    "for i_episode in xrange(3000):\n",
    "    observation = env.reset()\n",
    "\n",
    "    cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "    state = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                     to_bin(pole_angle, pole_angle_bins),\n",
    "                     to_bin(cart_velocity, cart_velocity_bins),\n",
    "                     to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "\n",
    "    for t in xrange(max_number_of_steps):\n",
    "        # env.render()\n",
    "\n",
    "        # Pick an action based on the current state\n",
    "        action = qlearn.chooseAction(state)\n",
    "        # Execute the action and get feedback\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Digitize the observation to get a state\n",
    "        cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "        nextState = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                         to_bin(pole_angle, pole_angle_bins),\n",
    "                         to_bin(cart_velocity, cart_velocity_bins),\n",
    "                         to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "\n",
    "        # # If out of bounds\n",
    "        # if (cart_position > 2.4 or cart_position < -2.4):\n",
    "        #     reward = -200\n",
    "        #     qlearn.learn(state, action, reward, nextState)\n",
    "        #     print(\"Out of bounds, reseting\")\n",
    "        #     break\n",
    "\n",
    "        if not(done):\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "            state = nextState\n",
    "        else:\n",
    "            # Q-learn stuff\n",
    "            reward = -200\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "            last_time_steps = numpy.append(last_time_steps, [int(t + 1)])\n",
    "            break\n",
    "\n",
    "l = last_time_steps.tolist()\n",
    "l.sort()\n",
    "print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "print(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "env.close()\n",
    "# gym.upload('/tmp/cartpole-experiment-1', algorithm_id='vmayoral simple Q-learning', api_key='your-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-10-03 12:47:01,080] Making new env: CartPole-v0\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:157: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"lecun_uniform\", input_shape=(4,))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:166: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"lecun_uniform\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:171: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, kernel_initializer=\"lecun_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               1500      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 92,402\n",
      "Trainable params: 92,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 300)               1500      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 92,402\n",
      "Trainable params: 92,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1\n",
      "decrease reward\n",
      "Episode  0  finished after 24 timesteps\n",
      "0.995\n",
      "decrease reward\n",
      "Episode  1  finished after 33 timesteps\n",
      "0.990025\n",
      "decrease reward\n",
      "Episode  2  finished after 9 timesteps\n",
      "0.985074875\n",
      "decrease reward\n",
      "Episode  3  finished after 17 timesteps\n",
      "0.980149500625\n",
      "decrease reward\n",
      "Episode  4  finished after 13 timesteps\n",
      "0.975248753122\n",
      "decrease reward\n",
      "Episode  5  finished after 13 timesteps\n",
      "0.970372509356\n",
      "decrease reward\n",
      "Episode  6  finished after 22 timesteps\n",
      "0.965520646809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  7  finished after 12 timesteps\n",
      "0.960693043575\n",
      "decrease reward\n",
      "Episode  8  finished after 14 timesteps\n",
      "0.955889578358\n",
      "decrease reward\n",
      "Episode  9  finished after 12 timesteps\n",
      "0.951110130466\n",
      "decrease reward\n",
      "Episode  10  finished after 10 timesteps\n",
      "0.946354579813\n",
      "decrease reward\n",
      "Episode  11  finished after 20 timesteps\n",
      "0.941622806914\n",
      "decrease reward\n",
      "Episode  12  finished after 16 timesteps\n",
      "0.93691469288\n",
      "decrease reward\n",
      "Episode  13  finished after 18 timesteps\n",
      "0.932230119415\n",
      "decrease reward\n",
      "Episode  14  finished after 13 timesteps\n",
      "0.927568968818\n",
      "decrease reward\n",
      "Episode  15  finished after 15 timesteps\n",
      "0.922931123974\n",
      "decrease reward\n",
      "Episode  16  finished after 28 timesteps\n",
      "0.918316468354\n",
      "decrease reward\n",
      "Episode  17  finished after 42 timesteps\n",
      "0.913724886013\n",
      "decrease reward\n",
      "Episode  18  finished after 15 timesteps\n",
      "0.909156261583\n",
      "decrease reward\n",
      "Episode  19  finished after 13 timesteps\n",
      "0.904610480275\n",
      "decrease reward\n",
      "Episode  20  finished after 15 timesteps\n",
      "0.900087427873\n",
      "decrease reward\n",
      "Episode  21  finished after 13 timesteps\n",
      "0.895586990734\n",
      "decrease reward\n",
      "Episode  22  finished after 27 timesteps\n",
      "0.89110905578\n",
      "decrease reward\n",
      "Episode  23  finished after 22 timesteps\n",
      "0.886653510501\n",
      "decrease reward\n",
      "Episode  24  finished after 16 timesteps\n",
      "0.882220242949\n",
      "decrease reward\n",
      "Episode  25  finished after 15 timesteps\n",
      "0.877809141734\n",
      "decrease reward\n",
      "Episode  26  finished after 28 timesteps\n",
      "0.873420096025\n",
      "decrease reward\n",
      "Episode  27  finished after 14 timesteps\n",
      "0.869052995545\n",
      "decrease reward\n",
      "Episode  28  finished after 20 timesteps\n",
      "0.864707730568\n",
      "decrease reward\n",
      "Episode  29  finished after 38 timesteps\n",
      "0.860384191915\n",
      "decrease reward\n",
      "Episode  30  finished after 17 timesteps\n",
      "0.856082270955\n",
      "decrease reward\n",
      "Episode  31  finished after 17 timesteps\n",
      "0.8518018596\n",
      "decrease reward\n",
      "Episode  32  finished after 16 timesteps\n",
      "0.847542850302\n",
      "decrease reward\n",
      "Episode  33  finished after 31 timesteps\n",
      "0.843305136051\n",
      "decrease reward\n",
      "Episode  34  finished after 21 timesteps\n",
      "0.839088610371\n",
      "decrease reward\n",
      "Episode  35  finished after 63 timesteps\n",
      "0.834893167319\n",
      "decrease reward\n",
      "Episode  36  finished after 34 timesteps\n",
      "0.830718701482\n",
      "decrease reward\n",
      "Episode  37  finished after 32 timesteps\n",
      "0.826565107975\n",
      "decrease reward\n",
      "Episode  38  finished after 12 timesteps\n",
      "0.822432282435\n",
      "decrease reward\n",
      "Episode  39  finished after 80 timesteps\n",
      "0.818320121023\n",
      "decrease reward\n",
      "Episode  40  finished after 57 timesteps\n",
      "0.814228520418\n",
      "decrease reward\n",
      "Episode  41  finished after 16 timesteps\n",
      "0.810157377815\n",
      "decrease reward\n",
      "Episode  42  finished after 18 timesteps\n",
      "0.806106590926\n",
      "decrease reward\n",
      "Episode  43  finished after 78 timesteps\n",
      "0.802076057972\n",
      "decrease reward\n",
      "Episode  44  finished after 63 timesteps\n",
      "0.798065677682\n",
      "decrease reward\n",
      "Episode  45  finished after 13 timesteps\n",
      "0.794075349293\n",
      "decrease reward\n",
      "Episode  46  finished after 61 timesteps\n",
      "0.790104972547\n",
      "decrease reward\n",
      "Episode  47  finished after 37 timesteps\n",
      "0.786154447684\n",
      "decrease reward\n",
      "Episode  48  finished after 57 timesteps\n",
      "0.782223675446\n",
      "decrease reward\n",
      "Episode  49  finished after 30 timesteps\n",
      "0.778312557069\n",
      "decrease reward\n",
      "Episode  50  finished after 25 timesteps\n",
      "0.774420994283\n",
      "decrease reward\n",
      "Episode  51  finished after 28 timesteps\n",
      "0.770548889312\n",
      "decrease reward\n",
      "Episode  52  finished after 36 timesteps\n",
      "0.766696144865\n",
      "decrease reward\n",
      "Episode  53  finished after 12 timesteps\n",
      "0.762862664141\n",
      "decrease reward\n",
      "Episode  54  finished after 73 timesteps\n",
      "0.75904835082\n",
      "decrease reward\n",
      "Episode  55  finished after 12 timesteps\n",
      "0.755253109066\n",
      "decrease reward\n",
      "Episode  56  finished after 25 timesteps\n",
      "0.751476843521\n",
      "decrease reward\n",
      "Episode  57  finished after 44 timesteps\n",
      "0.747719459303\n",
      "decrease reward\n",
      "Episode  58  finished after 27 timesteps\n",
      "0.743980862007\n",
      "decrease reward\n",
      "Episode  59  finished after 141 timesteps\n",
      "0.740260957697\n",
      "decrease reward\n",
      "Episode  60  finished after 17 timesteps\n",
      "0.736559652908\n",
      "decrease reward\n",
      "Episode  61  finished after 36 timesteps\n",
      "0.732876854644\n",
      "decrease reward\n",
      "Episode  62  finished after 91 timesteps\n",
      "0.72921247037\n",
      "decrease reward\n",
      "Episode  63  finished after 22 timesteps\n",
      "0.725566408019\n",
      "decrease reward\n",
      "Episode  64  finished after 65 timesteps\n",
      "0.721938575979\n",
      "decrease reward\n",
      "Episode  65  finished after 31 timesteps\n",
      "0.718328883099\n",
      "decrease reward\n",
      "Episode  66  finished after 54 timesteps\n",
      "0.714737238683\n",
      "decrease reward\n",
      "Episode  67  finished after 78 timesteps\n",
      "0.71116355249\n",
      "decrease reward\n",
      "Episode  68  finished after 35 timesteps\n",
      "0.707607734727\n",
      "decrease reward\n",
      "Episode  69  finished after 20 timesteps\n",
      "0.704069696054\n",
      "decrease reward\n",
      "Episode  70  finished after 13 timesteps\n",
      "0.700549347573\n",
      "decrease reward\n",
      "Episode  71  finished after 19 timesteps\n",
      "0.697046600835\n",
      "decrease reward\n",
      "Episode  72  finished after 10 timesteps\n",
      "0.693561367831\n",
      "decrease reward\n",
      "Episode  73  finished after 34 timesteps\n",
      "0.690093560992\n",
      "decrease reward\n",
      "Episode  74  finished after 103 timesteps\n",
      "0.686643093187\n",
      "decrease reward\n",
      "Episode  75  finished after 46 timesteps\n",
      "0.683209877721\n",
      "decrease reward\n",
      "Episode  76  finished after 115 timesteps\n",
      "0.679793828333\n",
      "decrease reward\n",
      "Episode  77  finished after 92 timesteps\n",
      "0.676394859191\n",
      "decrease reward\n",
      "Episode  78  finished after 41 timesteps\n",
      "0.673012884895\n",
      "decrease reward\n",
      "Episode  79  finished after 19 timesteps\n",
      "0.669647820471\n",
      "decrease reward\n",
      "Episode  80  finished after 137 timesteps\n",
      "0.666299581368\n",
      "decrease reward\n",
      "Episode  81  finished after 17 timesteps\n",
      "0.662968083461\n",
      "decrease reward\n",
      "Episode  82  finished after 84 timesteps\n",
      "0.659653243044\n",
      "decrease reward\n",
      "Episode  83  finished after 50 timesteps\n",
      "0.656354976829\n",
      "decrease reward\n",
      "Episode  84  finished after 101 timesteps\n",
      "0.653073201945\n",
      "decrease reward\n",
      "Episode  85  finished after 170 timesteps\n",
      "0.649807835935\n",
      "decrease reward\n",
      "Episode  86  finished after 33 timesteps\n",
      "0.646558796755\n",
      "decrease reward\n",
      "Episode  87  finished after 31 timesteps\n",
      "0.643326002772\n",
      "decrease reward\n",
      "Episode  88  finished after 57 timesteps\n",
      "0.640109372758\n",
      "decrease reward\n",
      "Episode  89  finished after 12 timesteps\n",
      "0.636908825894\n",
      "decrease reward\n",
      "Episode  90  finished after 42 timesteps\n",
      "0.633724281764\n",
      "decrease reward\n",
      "Episode  91  finished after 119 timesteps\n",
      "0.630555660356\n",
      "decrease reward\n",
      "Episode  92  finished after 156 timesteps\n",
      "0.627402882054\n",
      "decrease reward\n",
      "Episode  93  finished after 69 timesteps\n",
      "0.624265867644\n",
      "decrease reward\n",
      "Episode  94  finished after 47 timesteps\n",
      "0.621144538305\n",
      "reached the end! :D\n",
      "Episode  95  finished after 200 timesteps\n",
      "0.618038815614\n",
      "decrease reward\n",
      "Episode  96  finished after 63 timesteps\n",
      "0.614948621536\n",
      "decrease reward\n",
      "Episode  97  finished after 76 timesteps\n",
      "0.611873878428\n",
      "decrease reward\n",
      "Episode  98  finished after 125 timesteps\n",
      "0.608814509036\n",
      "decrease reward\n",
      "Episode  99  finished after 100 timesteps  last 100 average:  42\n",
      "0.605770436491\n",
      "decrease reward\n",
      "Episode  100  finished after 37 timesteps  last 100 average:  42\n",
      "0.602741584308\n",
      "decrease reward\n",
      "Episode  101  finished after 139 timesteps  last 100 average:  43\n",
      "0.599727876387\n",
      "decrease reward\n",
      "Episode  102  finished after 72 timesteps  last 100 average:  44\n",
      "0.596729237005\n",
      "decrease reward\n",
      "Episode  103  finished after 25 timesteps  last 100 average:  44\n",
      "0.59374559082\n",
      "decrease reward\n",
      "Episode  104  finished after 46 timesteps  last 100 average:  44\n",
      "0.590776862866\n",
      "decrease reward\n",
      "Episode  105  finished after 63 timesteps  last 100 average:  45\n",
      "0.587822978551\n",
      "decrease reward\n",
      "Episode  106  finished after 16 timesteps  last 100 average:  45\n",
      "0.584883863659\n",
      "decrease reward\n",
      "Episode  107  finished after 24 timesteps  last 100 average:  45\n",
      "0.58195944434\n",
      "decrease reward\n",
      "Episode  108  finished after 124 timesteps  last 100 average:  46\n",
      "0.579049647119\n",
      "decrease reward\n",
      "Episode  109  finished after 24 timesteps  last 100 average:  46\n",
      "0.576154398883\n",
      "decrease reward\n",
      "Episode  110  finished after 139 timesteps  last 100 average:  48\n",
      "0.573273626889\n",
      "decrease reward\n",
      "Episode  111  finished after 36 timesteps  last 100 average:  48\n",
      "0.570407258754\n",
      "decrease reward\n",
      "Episode  112  finished after 42 timesteps  last 100 average:  48\n",
      "0.56755522246\n",
      "decrease reward\n",
      "Episode  113  finished after 129 timesteps  last 100 average:  49\n",
      "0.564717446348\n",
      "decrease reward\n",
      "Episode  114  finished after 15 timesteps  last 100 average:  49\n",
      "0.561893859116\n",
      "decrease reward\n",
      "Episode  115  finished after 19 timesteps  last 100 average:  49\n",
      "0.559084389821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  116  finished after 20 timesteps  last 100 average:  49\n",
      "0.556288967872\n",
      "reached the end! :D\n",
      "Episode  117  finished after 200 timesteps  last 100 average:  51\n",
      "0.553507523032\n",
      "decrease reward\n",
      "Episode  118  finished after 51 timesteps  last 100 average:  51\n",
      "0.550739985417\n",
      "decrease reward\n",
      "Episode  119  finished after 177 timesteps  last 100 average:  53\n",
      "0.54798628549\n",
      "decrease reward\n",
      "Episode  120  finished after 35 timesteps  last 100 average:  53\n",
      "0.545246354063\n",
      "decrease reward\n",
      "Episode  121  finished after 29 timesteps  last 100 average:  53\n",
      "0.542520122292\n",
      "decrease reward\n",
      "Episode  122  finished after 17 timesteps  last 100 average:  53\n",
      "0.539807521681\n",
      "decrease reward\n",
      "Episode  123  finished after 137 timesteps  last 100 average:  54\n",
      "0.537108484072\n",
      "decrease reward\n",
      "Episode  124  finished after 135 timesteps  last 100 average:  55\n",
      "0.534422941652\n",
      "decrease reward\n",
      "Episode  125  finished after 153 timesteps  last 100 average:  57\n",
      "0.531750826944\n",
      "decrease reward\n",
      "Episode  126  finished after 127 timesteps  last 100 average:  58\n",
      "0.529092072809\n",
      "decrease reward\n",
      "Episode  127  finished after 116 timesteps  last 100 average:  59\n",
      "0.526446612445\n",
      "decrease reward\n",
      "Episode  128  finished after 161 timesteps  last 100 average:  60\n",
      "0.523814379383\n",
      "decrease reward\n",
      "Episode  129  finished after 37 timesteps  last 100 average:  60\n",
      "0.521195307486\n",
      "decrease reward\n",
      "Episode  130  finished after 32 timesteps  last 100 average:  60\n",
      "0.518589330948\n",
      "decrease reward\n",
      "Episode  131  finished after 10 timesteps  last 100 average:  60\n",
      "0.515996384294\n",
      "decrease reward\n",
      "Episode  132  finished after 101 timesteps  last 100 average:  61\n",
      "0.513416402372\n",
      "decrease reward\n",
      "Episode  133  finished after 150 timesteps  last 100 average:  62\n",
      "0.51084932036\n",
      "decrease reward\n",
      "Episode  134  finished after 144 timesteps  last 100 average:  63\n",
      "0.508295073759\n",
      "decrease reward\n",
      "Episode  135  finished after 145 timesteps  last 100 average:  64\n",
      "0.50575359839\n",
      "decrease reward\n",
      "Episode  136  finished after 150 timesteps  last 100 average:  65\n",
      "0.503224830398\n",
      "decrease reward\n",
      "Episode  137  finished after 134 timesteps  last 100 average:  66\n",
      "0.500708706246\n",
      "reached the end! :D\n",
      "Episode  138  finished after 200 timesteps  last 100 average:  68\n",
      "0.498205162715\n",
      "decrease reward\n",
      "Episode  139  finished after 127 timesteps  last 100 average:  69\n",
      "0.495714136901\n",
      "decrease reward\n",
      "Episode  140  finished after 143 timesteps  last 100 average:  70\n",
      "0.493235566217\n",
      "decrease reward\n",
      "Episode  141  finished after 180 timesteps  last 100 average:  71\n",
      "0.490769388385\n",
      "reached the end! :D\n",
      "Episode  142  finished after 200 timesteps  last 100 average:  73\n",
      "0.488315541444\n",
      "decrease reward\n",
      "Episode  143  finished after 178 timesteps  last 100 average:  74\n",
      "0.485873963736\n",
      "decrease reward\n",
      "Episode  144  finished after 36 timesteps  last 100 average:  74\n",
      "0.483444593918\n",
      "decrease reward\n",
      "Episode  145  finished after 31 timesteps  last 100 average:  74\n",
      "0.481027370948\n",
      "decrease reward\n",
      "Episode  146  finished after 32 timesteps  last 100 average:  74\n",
      "0.478622234093\n",
      "decrease reward\n",
      "Episode  147  finished after 146 timesteps  last 100 average:  75\n",
      "0.476229122923\n",
      "decrease reward\n",
      "Episode  148  finished after 132 timesteps  last 100 average:  75\n",
      "0.473847977308\n",
      "decrease reward\n",
      "Episode  149  finished after 36 timesteps  last 100 average:  76\n",
      "0.471478737422\n",
      "decrease reward\n",
      "Episode  150  finished after 155 timesteps  last 100 average:  77\n",
      "0.469121343735\n",
      "decrease reward\n",
      "Episode  151  finished after 55 timesteps  last 100 average:  77\n",
      "0.466775737016\n",
      "decrease reward\n",
      "Episode  152  finished after 183 timesteps  last 100 average:  79\n",
      "0.464441858331\n",
      "decrease reward\n",
      "Episode  153  finished after 116 timesteps  last 100 average:  80\n",
      "0.462119649039\n",
      "decrease reward\n",
      "Episode  154  finished after 145 timesteps  last 100 average:  80\n",
      "0.459809050794\n",
      "decrease reward\n",
      "Episode  155  finished after 26 timesteps  last 100 average:  80\n",
      "0.45751000554\n",
      "decrease reward\n",
      "Episode  156  finished after 67 timesteps  last 100 average:  81\n",
      "0.455222455512\n",
      "decrease reward\n",
      "Episode  157  finished after 122 timesteps  last 100 average:  82\n",
      "0.452946343235\n",
      "decrease reward\n",
      "Episode  158  finished after 131 timesteps  last 100 average:  83\n",
      "0.450681611519\n",
      "updating target network\n",
      "decrease reward\n",
      "Episode  159  finished after 175 timesteps  last 100 average:  83\n",
      "0.448428203461\n",
      "decrease reward\n",
      "Episode  160  finished after 121 timesteps  last 100 average:  84\n",
      "0.446186062444\n",
      "decrease reward\n",
      "Episode  161  finished after 170 timesteps  last 100 average:  85\n",
      "0.443955132131\n",
      "decrease reward\n",
      "Episode  162  finished after 105 timesteps  last 100 average:  86\n",
      "0.441735356471\n",
      "decrease reward\n",
      "Episode  163  finished after 151 timesteps  last 100 average:  87\n",
      "0.439526679688\n",
      "decrease reward\n",
      "Episode  164  finished after 173 timesteps  last 100 average:  88\n",
      "0.43732904629\n",
      "decrease reward\n",
      "Episode  165  finished after 159 timesteps  last 100 average:  89\n",
      "0.435142401059\n",
      "reached the end! :D\n",
      "Episode  166  finished after 200 timesteps  last 100 average:  91\n",
      "0.432966689053\n",
      "decrease reward\n",
      "Episode  167  finished after 156 timesteps  last 100 average:  91\n",
      "0.430801855608\n",
      "decrease reward\n",
      "Episode  168  finished after 25 timesteps  last 100 average:  91\n",
      "0.42864784633\n",
      "decrease reward\n",
      "Episode  169  finished after 170 timesteps  last 100 average:  93\n",
      "0.426504607098\n",
      "decrease reward\n",
      "Episode  170  finished after 126 timesteps  last 100 average:  94\n",
      "0.424372084063\n",
      "decrease reward\n",
      "Episode  171  finished after 111 timesteps  last 100 average:  95\n",
      "0.422250223642\n",
      "decrease reward\n",
      "Episode  172  finished after 121 timesteps  last 100 average:  96\n",
      "0.420138972524\n",
      "decrease reward\n",
      "Episode  173  finished after 177 timesteps  last 100 average:  97\n",
      "0.418038277662\n",
      "decrease reward\n",
      "Episode  174  finished after 145 timesteps  last 100 average:  98\n",
      "0.415948086273\n",
      "decrease reward\n",
      "Episode  175  finished after 137 timesteps  last 100 average:  99\n",
      "0.413868345842\n",
      "decrease reward\n",
      "Episode  176  finished after 196 timesteps  last 100 average:  100\n",
      "0.411799004113\n",
      "decrease reward\n",
      "Episode  177  finished after 131 timesteps  last 100 average:  100\n",
      "0.409740009092\n",
      "decrease reward\n",
      "Episode  178  finished after 142 timesteps  last 100 average:  101\n",
      "0.407691309047\n",
      "decrease reward\n",
      "Episode  179  finished after 150 timesteps  last 100 average:  102\n",
      "0.405652852502\n",
      "decrease reward\n",
      "Episode  180  finished after 23 timesteps  last 100 average:  101\n",
      "0.403624588239\n",
      "decrease reward\n",
      "Episode  181  finished after 150 timesteps  last 100 average:  103\n",
      "0.401606465298\n",
      "reached the end! :D\n",
      "Episode  182  finished after 200 timesteps  last 100 average:  104\n",
      "0.399598432971\n",
      "reached the end! :D\n",
      "Episode  183  finished after 200 timesteps  last 100 average:  105\n",
      "0.397600440806\n",
      "decrease reward\n",
      "Episode  184  finished after 104 timesteps  last 100 average:  105\n",
      "0.395612438602\n",
      "decrease reward\n",
      "Episode  185  finished after 58 timesteps  last 100 average:  104\n",
      "0.393634376409\n",
      "reached the end! :D\n",
      "Episode  186  finished after 200 timesteps  last 100 average:  106\n",
      "0.391666204527\n",
      "decrease reward\n",
      "Episode  187  finished after 141 timesteps  last 100 average:  107\n",
      "0.389707873505\n",
      "reached the end! :D\n",
      "Episode  188  finished after 200 timesteps  last 100 average:  108\n",
      "0.387759334137\n",
      "decrease reward\n",
      "Episode  189  finished after 175 timesteps  last 100 average:  110\n",
      "0.385820537467\n",
      "decrease reward\n",
      "Episode  190  finished after 166 timesteps  last 100 average:  111\n",
      "0.383891434779\n",
      "decrease reward\n",
      "Episode  191  finished after 135 timesteps  last 100 average:  111\n",
      "0.381971977605\n",
      "decrease reward\n",
      "Episode  192  finished after 74 timesteps  last 100 average:  110\n",
      "0.380062117717\n",
      "decrease reward\n",
      "Episode  193  finished after 160 timesteps  last 100 average:  111\n",
      "0.378161807129\n",
      "decrease reward\n",
      "Episode  194  finished after 171 timesteps  last 100 average:  113\n",
      "0.376270998093\n",
      "decrease reward\n",
      "Episode  195  finished after 123 timesteps  last 100 average:  112\n",
      "0.374389643103\n",
      "decrease reward\n",
      "Episode  196  finished after 129 timesteps  last 100 average:  113\n",
      "0.372517694887\n",
      "decrease reward\n",
      "Episode  197  finished after 119 timesteps  last 100 average:  113\n",
      "0.370655106413\n",
      "decrease reward\n",
      "Episode  198  finished after 154 timesteps  last 100 average:  113\n",
      "0.368801830881\n",
      "decrease reward\n",
      "Episode  199  finished after 123 timesteps  last 100 average:  113\n",
      "0.366957821726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  200  finished after 150 timesteps  last 100 average:  115\n",
      "0.365123032618\n",
      "decrease reward\n",
      "Episode  201  finished after 184 timesteps  last 100 average:  115\n",
      "0.363297417454\n",
      "reached the end! :D\n",
      "Episode  202  finished after 200 timesteps  last 100 average:  116\n",
      "0.361480930367\n",
      "decrease reward\n",
      "Episode  203  finished after 141 timesteps  last 100 average:  118\n",
      "0.359673525715\n",
      "reached the end! :D\n",
      "Episode  204  finished after 200 timesteps  last 100 average:  119\n",
      "0.357875158087\n",
      "decrease reward\n",
      "Episode  205  finished after 193 timesteps  last 100 average:  120\n",
      "0.356085782296\n",
      "decrease reward\n",
      "Episode  206  finished after 122 timesteps  last 100 average:  121\n",
      "0.354305353385\n",
      "decrease reward\n",
      "Episode  207  finished after 125 timesteps  last 100 average:  122\n",
      "0.352533826618\n",
      "decrease reward\n",
      "Episode  208  finished after 160 timesteps  last 100 average:  123\n",
      "0.350771157485\n",
      "decrease reward\n",
      "Episode  209  finished after 67 timesteps  last 100 average:  123\n",
      "0.349017301697\n",
      "decrease reward\n",
      "Episode  210  finished after 179 timesteps  last 100 average:  124\n",
      "0.347272215189\n",
      "decrease reward\n",
      "Episode  211  finished after 39 timesteps  last 100 average:  124\n",
      "0.345535854113\n",
      "decrease reward\n",
      "Episode  212  finished after 133 timesteps  last 100 average:  125\n",
      "0.343808174842\n",
      "decrease reward\n",
      "Episode  213  finished after 191 timesteps  last 100 average:  125\n",
      "0.342089133968\n",
      "decrease reward\n",
      "Episode  214  finished after 144 timesteps  last 100 average:  126\n",
      "0.340378688298\n",
      "decrease reward\n",
      "Episode  215  finished after 159 timesteps  last 100 average:  128\n",
      "0.338676794857\n",
      "decrease reward\n",
      "Episode  216  finished after 194 timesteps  last 100 average:  130\n",
      "0.336983410883\n",
      "decrease reward\n",
      "Episode  217  finished after 126 timesteps  last 100 average:  129\n",
      "0.335298493828\n",
      "decrease reward\n",
      "Episode  218  finished after 194 timesteps  last 100 average:  130\n",
      "0.333622001359\n",
      "decrease reward\n",
      "Episode  219  finished after 143 timesteps  last 100 average:  130\n",
      "0.331953891352\n",
      "decrease reward\n",
      "Episode  220  finished after 168 timesteps  last 100 average:  131\n",
      "0.330294121895\n",
      "decrease reward\n",
      "Episode  221  finished after 145 timesteps  last 100 average:  132\n",
      "0.328642651286\n",
      "reached the end! :D\n",
      "Episode  222  finished after 200 timesteps  last 100 average:  134\n",
      "0.32699943803\n",
      "reached the end! :D\n",
      "Episode  223  finished after 200 timesteps  last 100 average:  135\n",
      "0.325364440839\n",
      "decrease reward\n",
      "Episode  224  finished after 181 timesteps  last 100 average:  135\n",
      "0.323737618635\n",
      "decrease reward\n",
      "Episode  225  finished after 96 timesteps  last 100 average:  135\n",
      "0.322118930542\n",
      "reached the end! :D\n",
      "Episode  226  finished after 200 timesteps  last 100 average:  136\n",
      "0.320508335889\n",
      "updating target network\n",
      "decrease reward\n",
      "Episode  227  finished after 165 timesteps  last 100 average:  136\n",
      "0.31890579421\n",
      "decrease reward\n",
      "Episode  228  finished after 145 timesteps  last 100 average:  136\n",
      "0.317311265239\n",
      "decrease reward\n",
      "Episode  229  finished after 166 timesteps  last 100 average:  137\n",
      "0.315724708913\n",
      "decrease reward\n",
      "Episode  230  finished after 55 timesteps  last 100 average:  137\n",
      "0.314146085368\n",
      "reached the end! :D\n",
      "Episode  231  finished after 200 timesteps  last 100 average:  139\n",
      "0.312575354941\n",
      "reached the end! :D\n",
      "Episode  232  finished after 200 timesteps  last 100 average:  140\n",
      "0.311012478167\n",
      "decrease reward\n",
      "Episode  233  finished after 33 timesteps  last 100 average:  139\n",
      "0.309457415776\n",
      "decrease reward\n",
      "Episode  234  finished after 161 timesteps  last 100 average:  139\n",
      "0.307910128697\n",
      "reached the end! :D\n",
      "Episode  235  finished after 200 timesteps  last 100 average:  140\n",
      "0.306370578053\n",
      "decrease reward\n",
      "Episode  236  finished after 39 timesteps  last 100 average:  139\n",
      "0.304838725163\n",
      "decrease reward\n",
      "Episode  237  finished after 178 timesteps  last 100 average:  139\n",
      "0.303314531537\n",
      "decrease reward\n",
      "Episode  238  finished after 150 timesteps  last 100 average:  139\n",
      "0.30179795888\n",
      "decrease reward\n",
      "Episode  239  finished after 164 timesteps  last 100 average:  139\n",
      "0.300288969085\n",
      "reached the end! :D\n",
      "Episode  240  finished after 200 timesteps  last 100 average:  140\n",
      "0.29878752424\n",
      "decrease reward\n",
      "Episode  241  finished after 161 timesteps  last 100 average:  139\n",
      "0.297293586619\n",
      "decrease reward\n",
      "Episode  242  finished after 141 timesteps  last 100 average:  139\n",
      "0.295807118685\n",
      "reached the end! :D\n",
      "Episode  243  finished after 200 timesteps  last 100 average:  139\n",
      "0.294328083092\n",
      "reached the end! :D\n",
      "Episode  244  finished after 200 timesteps  last 100 average:  141\n",
      "0.292856442677\n",
      "decrease reward\n",
      "Episode  245  finished after 142 timesteps  last 100 average:  142\n",
      "0.291392160463\n",
      "decrease reward\n",
      "Episode  246  finished after 182 timesteps  last 100 average:  143\n",
      "0.289935199661\n",
      "decrease reward\n",
      "Episode  247  finished after 174 timesteps  last 100 average:  144\n",
      "0.288485523663\n",
      "decrease reward\n",
      "Episode  248  finished after 163 timesteps  last 100 average:  144\n",
      "0.287043096044\n",
      "decrease reward\n",
      "Episode  249  finished after 156 timesteps  last 100 average:  145\n",
      "0.285607880564\n",
      "decrease reward\n",
      "Episode  250  finished after 177 timesteps  last 100 average:  145\n",
      "0.284179841161\n",
      "decrease reward\n",
      "Episode  251  finished after 192 timesteps  last 100 average:  147\n",
      "0.282758941955\n",
      "decrease reward\n",
      "Episode  252  finished after 144 timesteps  last 100 average:  146\n",
      "0.281345147246\n",
      "decrease reward\n",
      "Episode  253  finished after 138 timesteps  last 100 average:  146\n",
      "0.279938421509\n",
      "decrease reward\n",
      "Episode  254  finished after 197 timesteps  last 100 average:  147\n",
      "0.278538729402\n",
      "decrease reward\n",
      "Episode  255  finished after 161 timesteps  last 100 average:  148\n",
      "0.277146035755\n",
      "decrease reward\n",
      "Episode  256  finished after 162 timesteps  last 100 average:  149\n",
      "0.275760305576\n",
      "decrease reward\n",
      "Episode  257  finished after 161 timesteps  last 100 average:  150\n",
      "0.274381504048\n",
      "decrease reward\n",
      "Episode  258  finished after 189 timesteps  last 100 average:  150\n",
      "0.273009596528\n",
      "decrease reward\n",
      "Episode  259  finished after 176 timesteps  last 100 average:  150\n",
      "0.271644548545\n",
      "reached the end! :D\n",
      "Episode  260  finished after 200 timesteps  last 100 average:  151\n",
      "0.270286325803\n",
      "decrease reward\n",
      "Episode  261  finished after 189 timesteps  last 100 average:  151\n",
      "0.268934894174\n",
      "reached the end! :D\n",
      "Episode  262  finished after 200 timesteps  last 100 average:  152\n",
      "0.267590219703\n",
      "decrease reward\n",
      "Episode  263  finished after 172 timesteps  last 100 average:  152\n",
      "0.266252268604\n",
      "decrease reward\n",
      "Episode  264  finished after 183 timesteps  last 100 average:  153\n",
      "0.264921007261\n",
      "reached the end! :D\n",
      "Episode  265  finished after 200 timesteps  last 100 average:  153\n",
      "0.263596402225\n",
      "decrease reward\n",
      "Episode  266  finished after 150 timesteps  last 100 average:  152\n",
      "0.262278420214\n",
      "reached the end! :D\n",
      "Episode  267  finished after 200 timesteps  last 100 average:  153\n",
      "0.260967028113\n",
      "reached the end! :D\n",
      "Episode  268  finished after 200 timesteps  last 100 average:  155\n",
      "0.259662192972\n",
      "reached the end! :D\n",
      "Episode  269  finished after 200 timesteps  last 100 average:  155\n",
      "0.258363882007\n",
      "reached the end! :D\n",
      "Episode  270  finished after 200 timesteps  last 100 average:  156\n",
      "0.257072062597\n",
      "reached the end! :D\n",
      "Episode  271  finished after 200 timesteps  last 100 average:  157\n",
      "0.255786702284\n",
      "decrease reward\n",
      "Episode  272  finished after 153 timesteps  last 100 average:  157\n",
      "0.254507768773\n",
      "decrease reward\n",
      "Episode  273  finished after 161 timesteps  last 100 average:  157\n",
      "0.253235229929\n",
      "decrease reward\n",
      "Episode  274  finished after 166 timesteps  last 100 average:  157\n",
      "0.251969053779\n",
      "decrease reward\n",
      "Episode  275  finished after 169 timesteps  last 100 average:  157\n",
      "0.25070920851\n",
      "reached the end! :D\n",
      "Episode  276  finished after 200 timesteps  last 100 average:  157\n",
      "0.249455662468\n",
      "reached the end! :D\n",
      "Episode  277  finished after 200 timesteps  last 100 average:  158\n",
      "0.248208384156\n",
      "decrease reward\n",
      "Episode  278  finished after 163 timesteps  last 100 average:  158\n",
      "0.246967342235\n",
      "decrease reward\n",
      "Episode  279  finished after 188 timesteps  last 100 average:  159\n",
      "0.245732505524\n",
      "reached the end! :D\n",
      "Episode  280  finished after 200 timesteps  last 100 average:  160\n",
      "0.244503842996\n",
      "decrease reward\n",
      "Episode  281  finished after 171 timesteps  last 100 average:  161\n",
      "0.243281323781\n",
      "reached the end! :D\n",
      "Episode  282  finished after 200 timesteps  last 100 average:  161\n",
      "0.242064917162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  283  finished after 165 timesteps  last 100 average:  160\n",
      "0.240854592576\n",
      "decrease reward\n",
      "Episode  284  finished after 165 timesteps  last 100 average:  161\n",
      "0.239650319613\n",
      "reached the end! :D\n",
      "Episode  285  finished after 200 timesteps  last 100 average:  162\n",
      "0.238452068015\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  286  finished after 200 timesteps  last 100 average:  162\n",
      "0.237259807675\n",
      "decrease reward\n",
      "Episode  287  finished after 166 timesteps  last 100 average:  162\n",
      "0.236073508637\n",
      "decrease reward\n",
      "Episode  288  finished after 163 timesteps  last 100 average:  162\n",
      "0.234893141094\n",
      "decrease reward\n",
      "Episode  289  finished after 150 timesteps  last 100 average:  162\n",
      "0.233718675388\n",
      "decrease reward\n",
      "Episode  290  finished after 181 timesteps  last 100 average:  162\n",
      "0.232550082011\n",
      "decrease reward\n",
      "Episode  291  finished after 184 timesteps  last 100 average:  162\n",
      "0.231387331601\n",
      "reached the end! :D\n",
      "Episode  292  finished after 200 timesteps  last 100 average:  164\n",
      "0.230230394943\n",
      "decrease reward\n",
      "Episode  293  finished after 180 timesteps  last 100 average:  164\n",
      "0.229079242968\n",
      "decrease reward\n",
      "Episode  294  finished after 171 timesteps  last 100 average:  164\n",
      "0.227933846754\n",
      "decrease reward\n",
      "Episode  295  finished after 185 timesteps  last 100 average:  165\n",
      "0.22679417752\n",
      "decrease reward\n",
      "Episode  296  finished after 188 timesteps  last 100 average:  165\n",
      "0.225660206632\n",
      "decrease reward\n",
      "Episode  297  finished after 181 timesteps  last 100 average:  166\n",
      "0.224531905599\n",
      "reached the end! :D\n",
      "Episode  298  finished after 200 timesteps  last 100 average:  166\n",
      "0.223409246071\n",
      "reached the end! :D\n",
      "Episode  299  finished after 200 timesteps  last 100 average:  167\n",
      "0.222292199841\n",
      "decrease reward\n",
      "Episode  300  finished after 187 timesteps  last 100 average:  167\n",
      "0.221180738842\n",
      "reached the end! :D\n",
      "Episode  301  finished after 200 timesteps  last 100 average:  168\n",
      "0.220074835147\n",
      "decrease reward\n",
      "Episode  302  finished after 150 timesteps  last 100 average:  167\n",
      "0.218974460972\n",
      "reached the end! :D\n",
      "Episode  303  finished after 200 timesteps  last 100 average:  168\n",
      "0.217879588667\n",
      "decrease reward\n",
      "Episode  304  finished after 198 timesteps  last 100 average:  168\n",
      "0.216790190723\n",
      "decrease reward\n",
      "Episode  305  finished after 183 timesteps  last 100 average:  168\n",
      "0.21570623977\n",
      "reached the end! :D\n",
      "Episode  306  finished after 200 timesteps  last 100 average:  168\n",
      "0.214627708571\n",
      "decrease reward\n",
      "Episode  307  finished after 175 timesteps  last 100 average:  169\n",
      "0.213554570028\n",
      "decrease reward\n",
      "Episode  308  finished after 196 timesteps  last 100 average:  169\n",
      "0.212486797178\n",
      "reached the end! :D\n",
      "Episode  309  finished after 200 timesteps  last 100 average:  170\n",
      "0.211424363192\n",
      "decrease reward\n",
      "Episode  310  finished after 103 timesteps  last 100 average:  170\n",
      "0.210367241376\n",
      "reached the end! :D\n",
      "Episode  311  finished after 200 timesteps  last 100 average:  171\n",
      "0.209315405169\n",
      "reached the end! :D\n",
      "Episode  312  finished after 200 timesteps  last 100 average:  172\n",
      "0.208268828143\n",
      "reached the end! :D\n",
      "Episode  313  finished after 200 timesteps  last 100 average:  172\n",
      "0.207227484003\n",
      "decrease reward\n",
      "Episode  314  finished after 75 timesteps  last 100 average:  171\n",
      "0.206191346583\n",
      "reached the end! :D\n",
      "Episode  315  finished after 200 timesteps  last 100 average:  172\n",
      "0.20516038985\n",
      "decrease reward\n",
      "Episode  316  finished after 160 timesteps  last 100 average:  171\n",
      "0.2041345879\n",
      "decrease reward\n",
      "Episode  317  finished after 185 timesteps  last 100 average:  172\n",
      "0.203113914961\n",
      "decrease reward\n",
      "Episode  318  finished after 151 timesteps  last 100 average:  172\n",
      "0.202098345386\n",
      "decrease reward\n",
      "Episode  319  finished after 174 timesteps  last 100 average:  172\n",
      "0.201087853659\n",
      "reached the end! :D\n",
      "Episode  320  finished after 200 timesteps  last 100 average:  172\n",
      "0.200082414391\n",
      "decrease reward\n",
      "Episode  321  finished after 171 timesteps  last 100 average:  173\n",
      "0.199082002319\n",
      "decrease reward\n",
      "Episode  322  finished after 188 timesteps  last 100 average:  172\n",
      "0.198086592307\n",
      "decrease reward\n",
      "Episode  323  finished after 191 timesteps  last 100 average:  172\n",
      "0.197096159346\n",
      "reached the end! :D\n",
      "Episode  324  finished after 200 timesteps  last 100 average:  172\n",
      "0.196110678549\n",
      "decrease reward\n",
      "Episode  325  finished after 176 timesteps  last 100 average:  173\n",
      "0.195130125156\n",
      "reached the end! :D\n",
      "Episode  326  finished after 200 timesteps  last 100 average:  173\n",
      "0.194154474531\n",
      "decrease reward\n",
      "Episode  327  finished after 195 timesteps  last 100 average:  174\n",
      "0.193183702158\n",
      "reached the end! :D\n",
      "Episode  328  finished after 200 timesteps  last 100 average:  174\n",
      "0.192217783647\n",
      "decrease reward\n",
      "Episode  329  finished after 163 timesteps  last 100 average:  174\n",
      "0.191256694729\n",
      "reached the end! :D\n",
      "Episode  330  finished after 200 timesteps  last 100 average:  176\n",
      "0.190300411255\n",
      "decrease reward\n",
      "Episode  331  finished after 194 timesteps  last 100 average:  176\n",
      "0.189348909199\n",
      "reached the end! :D\n",
      "Episode  332  finished after 200 timesteps  last 100 average:  176\n",
      "0.188402164653\n",
      "reached the end! :D\n",
      "Episode  333  finished after 200 timesteps  last 100 average:  177\n",
      "0.18746015383\n",
      "decrease reward\n",
      "Episode  334  finished after 166 timesteps  last 100 average:  177\n",
      "0.186522853061\n",
      "decrease reward\n",
      "Episode  335  finished after 197 timesteps  last 100 average:  177\n",
      "0.185590238795\n",
      "reached the end! :D\n",
      "Episode  336  finished after 200 timesteps  last 100 average:  179\n",
      "0.184662287601\n",
      "reached the end! :D\n",
      "Episode  337  finished after 200 timesteps  last 100 average:  179\n",
      "0.183738976163\n",
      "decrease reward\n",
      "Episode  338  finished after 181 timesteps  last 100 average:  179\n",
      "0.182820281282\n",
      "reached the end! :D\n",
      "Episode  339  finished after 200 timesteps  last 100 average:  180\n",
      "0.181906179876\n",
      "reached the end! :D\n",
      "Episode  340  finished after 200 timesteps  last 100 average:  180\n",
      "0.180996648977\n",
      "updating target network\n",
      "reached the end! :D\n",
      "Episode  341  finished after 200 timesteps  last 100 average:  180\n",
      "0.180091665732\n",
      "decrease reward\n",
      "Episode  342  finished after 179 timesteps  last 100 average:  180\n",
      "0.179191207403\n",
      "reached the end! :D\n",
      "Episode  343  finished after 200 timesteps  last 100 average:  180\n",
      "0.178295251366\n",
      "reached the end! :D\n",
      "Episode  344  finished after 200 timesteps  last 100 average:  180\n",
      "0.177403775109\n",
      "decrease reward\n",
      "Episode  345  finished after 165 timesteps  last 100 average:  181\n",
      "0.176516756234\n",
      "decrease reward\n",
      "Episode  346  finished after 195 timesteps  last 100 average:  181\n",
      "0.175634172453\n",
      "decrease reward\n",
      "Episode  347  finished after 188 timesteps  last 100 average:  181\n",
      "0.17475600159\n",
      "decrease reward\n",
      "Episode  348  finished after 151 timesteps  last 100 average:  181\n",
      "0.173882221582\n",
      "reached the end! :D\n",
      "Episode  349  finished after 200 timesteps  last 100 average:  181\n",
      "0.173012810474\n",
      "reached the end! :D\n",
      "Episode  350  finished after 200 timesteps  last 100 average:  182\n",
      "0.172147746422\n",
      "decrease reward\n",
      "Episode  351  finished after 174 timesteps  last 100 average:  181\n",
      "0.17128700769\n",
      "decrease reward\n",
      "Episode  352  finished after 182 timesteps  last 100 average:  182\n",
      "0.170430572652\n",
      "reached the end! :D\n",
      "Episode  353  finished after 200 timesteps  last 100 average:  182\n",
      "0.169578419788\n",
      "reached the end! :D\n",
      "Episode  354  finished after 200 timesteps  last 100 average:  182\n",
      "0.168730527689\n",
      "decrease reward\n",
      "Episode  355  finished after 162 timesteps  last 100 average:  182\n",
      "0.167886875051\n",
      "decrease reward\n",
      "Episode  356  finished after 167 timesteps  last 100 average:  182\n",
      "0.167047440676\n",
      "reached the end! :D\n",
      "Episode  357  finished after 200 timesteps  last 100 average:  183\n",
      "0.166212203472\n",
      "decrease reward\n",
      "Episode  358  finished after 187 timesteps  last 100 average:  183\n",
      "0.165381142455\n",
      "reached the end! :D\n",
      "Episode  359  finished after 200 timesteps  last 100 average:  183\n",
      "0.164554236743\n",
      "decrease reward\n",
      "Episode  360  finished after 191 timesteps  last 100 average:  183\n",
      "0.163731465559\n",
      "decrease reward\n",
      "Episode  361  finished after 175 timesteps  last 100 average:  183\n",
      "0.162912808231\n",
      "reached the end! :D\n",
      "Episode  362  finished after 200 timesteps  last 100 average:  183\n",
      "0.16209824419\n",
      "decrease reward\n",
      "Episode  363  finished after 186 timesteps  last 100 average:  183\n",
      "0.161287752969\n",
      "decrease reward\n",
      "Episode  364  finished after 178 timesteps  last 100 average:  183\n",
      "0.160481314204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease reward\n",
      "Episode  365  finished after 174 timesteps  last 100 average:  183\n",
      "0.159678907633\n",
      "decrease reward\n",
      "Episode  366  finished after 177 timesteps  last 100 average:  183\n",
      "0.158880513095\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Deep Q-learning approach to the cartpole problem\n",
    "using OpenAI's gym environment.\n",
    "\n",
    "As part of the basic series on reinforcement learning @\n",
    "https://github.com/vmayoral/basic_reinforcement_learning\n",
    "\n",
    "This code implements the algorithm described at:\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, \n",
    "S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.\n",
    "\n",
    "Code based on @wingedsheep's work at https://gist.github.com/wingedsheep/4199594b02138dd427c22a540d6d6b8d\n",
    "\n",
    "        @author: Victor Mayoral Vilches <victor@erlerobotics.com>\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# import os\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "# import theano\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    This class provides an abstraction to store the [s, a, r, a'] elements of each iteration.\n",
    "    Instead of using tuples (as other implementations do), the information is stored in lists \n",
    "    that get returned as another list of dictionaries with each key corresponding to either \n",
    "    \"state\", \"action\", \"reward\", \"nextState\" or \"isFinal\".\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.currentPosition = 0\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.newStates = []\n",
    "        self.finals = []\n",
    "\n",
    "    def getMiniBatch(self, size) :\n",
    "        indices = random.sample(np.arange(len(self.states)), min(size,len(self.states)) )\n",
    "        miniBatch = []\n",
    "        for index in indices:\n",
    "            miniBatch.append({'state': self.states[index],'action': self.actions[index], 'reward': self.rewards[index], 'newState': self.newStates[index], 'isFinal': self.finals[index]})\n",
    "        return miniBatch\n",
    "\n",
    "    def getCurrentSize(self) :\n",
    "        return len(self.states)\n",
    "\n",
    "    def getMemory(self, index): \n",
    "        return {'state': self.states[index],'action': self.actions[index], 'reward': self.rewards[index], 'newState': self.newStates[index], 'isFinal': self.finals[index]}\n",
    "\n",
    "    def addMemory(self, state, action, reward, newState, isFinal) :\n",
    "        if (self.currentPosition >= self.size - 1) :\n",
    "            self.currentPosition = 0\n",
    "        if (len(self.states) > self.size) :\n",
    "            self.states[self.currentPosition] = state\n",
    "            self.actions[self.currentPosition] = action\n",
    "            self.rewards[self.currentPosition] = reward\n",
    "            self.newStates[self.currentPosition] = newState\n",
    "            self.finals[self.currentPosition] = isFinal\n",
    "        else :\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.newStates.append(newState)\n",
    "            self.finals.append(isFinal)\n",
    "        \n",
    "        self.currentPosition += 1\n",
    "\n",
    "class DeepQ:\n",
    "    \"\"\"\n",
    "    DQN abstraction.\n",
    "\n",
    "    As a quick reminder:\n",
    "        traditional Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + gamma * max(Q(s') - Q(s,a))\n",
    "        DQN:\n",
    "            target = reward(s,a) + gamma * max(Q(s')\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, outputs, memorySize, discountFactor, learningRate, learnStart):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - inputs: input size\n",
    "            - outputs: output size\n",
    "            - memorySize: size of the memory that will store each state\n",
    "            - discountFactor: the discount factor (gamma)\n",
    "            - learningRate: learning rate\n",
    "            - learnStart: steps to happen before for learning. Set to 128\n",
    "        \"\"\"\n",
    "        self.input_size = inputs\n",
    "        self.output_size = outputs\n",
    "        self.memory = Memory(memorySize)\n",
    "        self.discountFactor = discountFactor\n",
    "        self.learnStart = learnStart\n",
    "        self.learningRate = learningRate\n",
    "   \n",
    "    def initNetworks(self, hiddenLayers):\n",
    "        model = self.createModel(self.input_size, self.output_size, hiddenLayers, \"relu\", self.learningRate)\n",
    "        self.model = model\n",
    "\n",
    "        targetModel = self.createModel(self.input_size, self.output_size, hiddenLayers, \"relu\", self.learningRate)\n",
    "        self.targetModel = targetModel\n",
    "\n",
    "    def createRegularizedModel(self, inputs, outputs, hiddenLayers, activationType, learningRate):\n",
    "        bias = True\n",
    "        dropout = 0\n",
    "        regularizationFactor = 0.01\n",
    "        model = Sequential()\n",
    "        if len(hiddenLayers) == 0: \n",
    "            model.add(Dense(self.output_size, input_shape=(self.input_size,), init='lecun_uniform', bias=bias))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        else :\n",
    "            if regularizationFactor > 0:\n",
    "                model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform', W_regularizer=l2(regularizationFactor),  bias=bias))\n",
    "            else:\n",
    "                model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform', bias=bias))\n",
    "\n",
    "            if (activationType == \"LeakyReLU\") :\n",
    "                model.add(LeakyReLU(alpha=0.01))\n",
    "            else :\n",
    "                model.add(Activation(activationType))\n",
    "            \n",
    "            for index in range(1, len(hiddenLayers)):\n",
    "                layerSize = hiddenLayers[index]\n",
    "                if regularizationFactor > 0:\n",
    "                    model.add(Dense(layerSize, init='lecun_uniform', W_regularizer=l2(regularizationFactor), bias=bias))\n",
    "                else:\n",
    "                    model.add(Dense(layerSize, init='lecun_uniform', bias=bias))\n",
    "                if (activationType == \"LeakyReLU\") :\n",
    "                    model.add(LeakyReLU(alpha=0.01))\n",
    "                else :\n",
    "                    model.add(Activation(activationType))\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "            model.add(Dense(self.output_size, init='lecun_uniform', bias=bias))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def createModel(self, inputs, outputs, hiddenLayers, activationType, learningRate):\n",
    "        model = Sequential()\n",
    "        if len(hiddenLayers) == 0: \n",
    "            model.add(Dense(self.output_size, input_shape=(self.input_size,), init='lecun_uniform'))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        else :\n",
    "            model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform'))\n",
    "            if (activationType == \"LeakyReLU\") :\n",
    "                model.add(LeakyReLU(alpha=0.01))\n",
    "            else :\n",
    "                model.add(Activation(activationType))\n",
    "            \n",
    "            for index in range(1, len(hiddenLayers)):\n",
    "                # print(\"adding layer \"+str(index))\n",
    "                layerSize = hiddenLayers[index]\n",
    "                model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "                if (activationType == \"LeakyReLU\") :\n",
    "                    model.add(LeakyReLU(alpha=0.01))\n",
    "                else :\n",
    "                    model.add(Activation(activationType))\n",
    "            model.add(Dense(self.output_size, init='lecun_uniform'))\n",
    "            model.add(Activation(\"linear\"))\n",
    "        optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def printNetwork(self):\n",
    "        i = 0\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            print \"layer \",i,\": \",weights\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    def backupNetwork(self, model, backup):\n",
    "        weightMatrix = []\n",
    "        for layer in model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            weightMatrix.append(weights)\n",
    "        i = 0\n",
    "        for layer in backup.layers:\n",
    "            weights = weightMatrix[i]\n",
    "            layer.set_weights(weights)\n",
    "            i += 1\n",
    "\n",
    "    def updateTargetNetwork(self):\n",
    "        self.backupNetwork(self.model, self.targetModel)\n",
    "\n",
    "    # predict Q values for all the actions\n",
    "    def getQValues(self, state):\n",
    "        predicted = self.model.predict(state.reshape(1,len(state)))\n",
    "        return predicted[0]\n",
    "\n",
    "    def getTargetQValues(self, state):\n",
    "        predicted = self.targetModel.predict(state.reshape(1,len(state)))\n",
    "        return predicted[0]\n",
    "\n",
    "    def getMaxQ(self, qValues):\n",
    "        return np.max(qValues)\n",
    "\n",
    "    def getMaxIndex(self, qValues):\n",
    "        return np.argmax(qValues)\n",
    "\n",
    "    # calculate the target function\n",
    "    def calculateTarget(self, qValuesNewState, reward, isFinal):\n",
    "        \"\"\"\n",
    "        target = reward(s,a) + gamma * max(Q(s')\n",
    "        \"\"\"\n",
    "        if isFinal:\n",
    "            return reward\n",
    "        else : \n",
    "            return reward + self.discountFactor * self.getMaxQ(qValuesNewState)\n",
    "\n",
    "    # select the action with the highest Q value\n",
    "    def selectAction(self, qValues, explorationRate):\n",
    "        rand = random.random()\n",
    "        if rand < explorationRate :\n",
    "            action = np.random.randint(0, self.output_size)\n",
    "        else :\n",
    "            action = self.getMaxIndex(qValues)\n",
    "        return action\n",
    "\n",
    "    def selectActionByProbability(self, qValues, bias):\n",
    "        qValueSum = 0\n",
    "        shiftBy = 0\n",
    "        for value in qValues:\n",
    "            if value + shiftBy < 0:\n",
    "                shiftBy = - (value + shiftBy)\n",
    "        shiftBy += 1e-06\n",
    "\n",
    "        for value in qValues:\n",
    "            qValueSum += (value + shiftBy) ** bias\n",
    "\n",
    "        probabilitySum = 0\n",
    "        qValueProbabilities = []\n",
    "        for value in qValues:\n",
    "            probability = ((value + shiftBy) ** bias) / float(qValueSum)\n",
    "            qValueProbabilities.append(probability + probabilitySum)\n",
    "            probabilitySum += probability\n",
    "        qValueProbabilities[len(qValueProbabilities) - 1] = 1\n",
    "\n",
    "        rand = random.random()\n",
    "        i = 0\n",
    "        for value in qValueProbabilities:\n",
    "            if (rand <= value):\n",
    "                return i\n",
    "            i += 1\n",
    "\n",
    "    def addMemory(self, state, action, reward, newState, isFinal):\n",
    "        self.memory.addMemory(state, action, reward, newState, isFinal)\n",
    "\n",
    "    def learnOnLastState(self):\n",
    "        if self.memory.getCurrentSize() >= 1:\n",
    "            return self.memory.getMemory(self.memory.getCurrentSize() - 1)\n",
    "\n",
    "    def learnOnMiniBatch(self, miniBatchSize, useTargetNetwork=True):\n",
    "        # Do not learn until we've got self.learnStart samples        \n",
    "        if self.memory.getCurrentSize() > self.learnStart:\n",
    "            # learn in batches of 128\n",
    "            miniBatch = self.memory.getMiniBatch(miniBatchSize)\n",
    "            X_batch = np.empty((0,self.input_size), dtype = np.float64)\n",
    "            Y_batch = np.empty((0,self.output_size), dtype = np.float64)\n",
    "            for sample in miniBatch:\n",
    "                isFinal = sample['isFinal']\n",
    "                state = sample['state']\n",
    "                action = sample['action']\n",
    "                reward = sample['reward']\n",
    "                newState = sample['newState']\n",
    "\n",
    "                qValues = self.getQValues(state)\n",
    "                if useTargetNetwork:\n",
    "                    qValuesNewState = self.getTargetQValues(newState)\n",
    "                else :\n",
    "                    qValuesNewState = self.getQValues(newState)\n",
    "                targetValue = self.calculateTarget(qValuesNewState, reward, isFinal)\n",
    "\n",
    "                X_batch = np.append(X_batch, np.array([state.copy()]), axis=0)\n",
    "                Y_sample = qValues.copy()\n",
    "                Y_sample[action] = targetValue\n",
    "                Y_batch = np.append(Y_batch, np.array([Y_sample]), axis=0)\n",
    "                if isFinal:\n",
    "                    X_batch = np.append(X_batch, np.array([newState.copy()]), axis=0)\n",
    "                    Y_batch = np.append(Y_batch, np.array([[reward]*self.output_size]), axis=0)\n",
    "            self.model.fit(X_batch, Y_batch, batch_size = len(miniBatch), nb_epoch=1, verbose = 0)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "epochs = 1000\n",
    "steps = 100000\n",
    "updateTargetNetwork = 10000\n",
    "explorationRate = 1\n",
    "minibatch_size = 128\n",
    "learnStart = 128\n",
    "learningRate = 0.00025\n",
    "discountFactor = 0.99\n",
    "memorySize = 1000000\n",
    "\n",
    "last100Scores = [0] * 100\n",
    "last100ScoresIndex = 0\n",
    "last100Filled = False\n",
    "\n",
    "deepQ = DeepQ(4, 2, memorySize, discountFactor, learningRate, learnStart)\n",
    "# deepQ.initNetworks([30,30,30])\n",
    "# deepQ.initNetworks([30,30])\n",
    "deepQ.initNetworks([300,300])\n",
    "\n",
    "stepCounter = 0\n",
    "\n",
    "# number of reruns\n",
    "for epoch in xrange(epochs):\n",
    "    observation = env.reset()\n",
    "    print explorationRate\n",
    "    # number of timesteps\n",
    "    for t in xrange(steps):\n",
    "        # env.render()\n",
    "        qValues = deepQ.getQValues(observation)\n",
    "\n",
    "        action = deepQ.selectAction(qValues, explorationRate)\n",
    "\n",
    "        newObservation, reward, done, info = env.step(action)\n",
    "\n",
    "        if (t >= 199):\n",
    "            print \"reached the end! :D\"\n",
    "            done = True\n",
    "            # reward = 200            \n",
    "\n",
    "        if done and t < 199:\n",
    "            print \"decrease reward\"\n",
    "            # reward -= 200\n",
    "        deepQ.addMemory(observation, action, reward, newObservation, done)\n",
    "\n",
    "        if stepCounter >= learnStart:\n",
    "            if stepCounter <= updateTargetNetwork:\n",
    "                deepQ.learnOnMiniBatch(minibatch_size, False)\n",
    "            else :\n",
    "                deepQ.learnOnMiniBatch(minibatch_size, True)\n",
    "\n",
    "        observation = newObservation\n",
    "\n",
    "        if done:\n",
    "            last100Scores[last100ScoresIndex] = t\n",
    "            last100ScoresIndex += 1\n",
    "            if last100ScoresIndex >= 100:\n",
    "                last100Filled = True\n",
    "                last100ScoresIndex = 0\n",
    "            if not last100Filled:\n",
    "                print \"Episode \",epoch,\" finished after {} timesteps\".format(t+1)\n",
    "            else :\n",
    "                print \"Episode \",epoch,\" finished after {} timesteps\".format(t+1),\" last 100 average: \",(sum(last100Scores)/len(last100Scores))\n",
    "            break\n",
    "\n",
    "        stepCounter += 1\n",
    "        if stepCounter % updateTargetNetwork == 0:\n",
    "            deepQ.updateTargetNetwork()\n",
    "            print \"updating target network\"\n",
    "\n",
    "    explorationRate *= 0.995\n",
    "    # explorationRate -= (2.0/epochs)\n",
    "    explorationRate = max (0.05, explorationRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
