{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font color=D00000>Q-learning в средах OpenAI.gym</font></center></h1>\n",
    "\n",
    "### <font color=black>Евгений Пономарев</font>\n",
    "#### <font color=green>Сколковский институт науки и технологий</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеки:\n",
    "* `pip install numpy`\n",
    "* `pip install tensorflow`\n",
    "* `pip install gym`\n",
    "* `pip install gym[atari]`\n",
    "* `pip install keras`\n",
    "* `pip install matplotlib`\n",
    "\n",
    "Документация к openai.gym:\n",
    "* https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><font color=D00000>Обучение с подкреплением aka Reinforcement learning:</font></center></h2>\n",
    " \n",
    "В классической постановке предполагается **Марковский процесс принятия решений (Markov Decision Process)**:\n",
    "\n",
    "Состоит из:\n",
    "* Среды\n",
    "* Агента\n",
    "    \n",
    "**MDP** задается через: \n",
    "* Множество состояний среды $\\mathrm{S} : s_t \\in \\mathrm{S}$\n",
    "* Доступные действия $\\mathrm{A}: a_t \\in \\mathrm{A}$\n",
    "* Модель среды (обычно неизвестна агенту): $s_{t+1} \\sim \\mathrm{P}_{env}(s_{t+1} | s_t, a_t)$\n",
    "* Функция награды (обычно неизвестна агенту): $r_{t} \\sim \\mathrm{R}(r_{t} | a_t, s_t)$\n",
    "  \n",
    "Агент совершает действия в соответствии с выработанной политикой:\n",
    "* $a_t \\sim \\pi(a_t|s_t)$\n",
    "\n",
    "<font color=green>Цель агента - максимизировать ожидание кумулятивной награды:\n",
    "$$R(\\pi) = \\mathbb{E}_{\\pi, P, R} \\sum_{t=start}^{end}{r_t} \\to \\max_{\\pi}$$\n",
    "    </font></center>\n",
    "\n",
    "<img src=\"http://pp.userapi.com/c621707/v621707705/23555/2WlxiB6Di3o.jpg\" alt=\"MDP\" style=\"width: 600px;\"/>\n",
    "\n",
    "Основной плюс - можно не размечать данные\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Табличный Q-learning\n",
    "\n",
    "Сопоставим каждой паре $(s,a)$ (состояние, действие) значение функции ценности действия - **$Q(s,a)$**\n",
    "$$Q(s_t, a_t) = \\mathbb{E}_{\\pi, P, R} \\sum_{i=0}^{N}{\\gamma^ir_{t+i}}$$\n",
    "\n",
    "Тогда награда за $$R(\\pi) = \\sum_{t=start}^{end}{r_t} \\sim \\sum_{t=start}^{end} Q(s_t|a_t)$$\n",
    "\n",
    "\n",
    "Искомая $Q$ функция:\n",
    "$$Q^*(s,a) = max_{\\pi}Q^{\\pi}(s,a)$$\n",
    "\n",
    "Тогда жадная политика\n",
    "$$ \\pi(a|s) = \\mathbb{[}a == \\arg\\max_a Q(s,a)\\mathbb{]} $$\n",
    "$$ a_t = \\arg\\max_a Q(s,a) $$\n",
    "решает исходную задачу\n",
    "\n",
    "Уравнение Беллмана для оптимальной функции:\n",
    "\n",
    "$$Q^*(s,a) = \\mathbb{E}_{P_{env}}(r_{t+1} + \\gamma \\max_{a'\\in A} Q^*(s_{t+1},a')~~|~~s_t = s, a_t = a)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Q(s_{t}, a_t) = r_t + \\gamma \\cdot \\max_{a' \\in A}Q(s_{t+1}|a')$$\n",
    "\n",
    "\n",
    "\n",
    "В простейшем случае, когда s~1,10,100; a~1,10 лучше просто выучить, что нужно делать, т.е. задать таблицу: \n",
    "\n",
    "| Q(s,a)| $s_1$ | $s_2$  | $s_3$ | ... |\n",
    "|-----|-----|------|-----|-----|\n",
    "| $a_1$ | 1   | 0    | 8   | ... |\n",
    "| $a_2$ | 5   | 4    | 3   | ... |\n",
    "| $a_3$ | 8   | -10 | 99  | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка $Q(s, a)$ экспоненциальным скользящим средним:\n",
    "\n",
    "\n",
    "$$ Q(s, a) := Q(s, a) + \\alpha \\cdot [(r(a,s) + \\max_{a'\\ \\in A} Q(s',a')) - Q(s,a)] $$\n",
    "\n",
    "$$ Q(s,a) \\to r(a,s) + \\max Q(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><font color=#D00000>CartPole</font></center></h2>\n",
    "\n",
    "![cartpole](cartpole_gif.gif)\n",
    "## Правила\n",
    "\n",
    "### Вектор наблюдения\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Позиция тележки | -2.4 | 2.4\n",
    "1 | Скорость тележки | -Inf | Inf\n",
    "2 | Угол наклона палки | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Угловая скорость палки | -Inf | Inf\n",
    "\n",
    "### Действия\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Толкнуть влево\n",
    "1 | Толкнуть вправо\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Награда\n",
    "1 за каждый шаг\n",
    "\n",
    "### Начальное состояние\n",
    "Случайные числа ±0.05\n",
    "\n",
    "### Завершение эпизода\n",
    "1. Наклон палки = ±20.9°\n",
    "2. Положение тележки = ±2.4 (центр тележки за пределами экрана)\n",
    "3. Eлительность эпизода 200 (v.0) или 500 (v.1)\n",
    "\n",
    "### Условия победы\n",
    "Продержаться 100 последовательных эпизодов со средним результатом более 195 (v.0) или 475 (v.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.q.get((state, a), 0.) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "        \n",
    "        # Отвечает за баланс исследование-использование (exploration vs expluatation)\n",
    "        # С вероятностью epsilon случайное действие, 1-epsilon - argmax(Q)\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # Если максимум Q достигается для двух и более действий - берем любое\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "        \n",
    "        action = self.actions[i]\n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.q.get((state, action), 0.) for a in self.actions])\n",
    "        new_q = reward + self.gamma*maxqnew\n",
    "\n",
    "        old_q = self.q.get((state, action), None)\n",
    "        if old_q is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = old_q + self.alpha * (new_q - old_q)\n",
    "\n",
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "    return np.digitize(x=[value], bins=bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "goal_average_steps = 195\n",
    "max_number_of_steps = 200\n",
    "# Для создания видео и отправки данных на сайт gym.openai.com\n",
    "env = gym.wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "n_bins = 8\n",
    "n_bins_angle = 10\n",
    "\n",
    "# Дискретизуем непрерывное пространство состояний в n_bins^2*n_bins_angle^2\n",
    "cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "# Инициализация табличного Q-learning\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "qlearn = QLearn(actions=range(env.action_space.n),\n",
    "                alpha=alpha, gamma=gamma, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C\n",
    "init s\n",
    "init Q\n",
    "eps = 0.1\n",
    "alpha = 0.5\n",
    "    \n",
    "for episode = 1,M do:\n",
    "    с вероятностью eps:\n",
    "        a = ramdom(A)\n",
    "    else:\n",
    "        a = argmax(Q[s,a])\n",
    "    \n",
    "    Совершить действие a;\n",
    "    Получить награду r;\n",
    "    Получить новое состояние s_new;\n",
    "    \n",
    "    Улучшить стратегию:\n",
    "        Q[s,a] := Q[s,a] + alpha*((r + max(Q[s_new,:])) - Q[s,a])\n",
    "    \n",
    "    s := s_new\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_reward = np.ndarray(0)\n",
    "for i_episode in xrange(30000):\n",
    "    \n",
    "    # получим наблюдение\n",
    "    observation = env.reset()\n",
    "    # дискретизуем и создадим состояние\n",
    "    cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "    state = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                     to_bin(pole_angle, pole_angle_bins),\n",
    "                     to_bin(cart_velocity, cart_velocity_bins),\n",
    "                     to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "    cum_reward = 0\n",
    "    for t in xrange(max_number_of_steps):\n",
    "        # можно смотреть, как агент учится\n",
    "#         if i_episode % 300 == 0: \n",
    "#             env.render()\n",
    "        \n",
    "        # в соответствии с политикой выберем действие\n",
    "        action = qlearn.chooseAction(state)\n",
    "        # получим новое наблюдение, награду, метку конца эпизода\n",
    "        observation, reward, is_done, _ = env.step(action)\n",
    "        cum_reward += reward\n",
    "        # дискретизуем и создадим состояние\n",
    "        cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n",
    "        new_state = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                         to_bin(pole_angle, pole_angle_bins),\n",
    "                         to_bin(cart_velocity, cart_velocity_bins),\n",
    "                         to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "\n",
    "\n",
    "        if not(is_done):\n",
    "            qlearn.learn(state, action, reward, new_state)\n",
    "            state = new_state\n",
    "        else:\n",
    "            # штраф за проигрыш\n",
    "            reward = -200\n",
    "            qlearn.learn(state, action, reward, new_state)\n",
    "            last_reward = np.append(last_reward, cum_reward)\n",
    "            break\n",
    "    print(\"episode: {}, score: {}, 30-mean: {:0.1f}\".format(i_episode, t, last_reward[-30:].mean()))\n",
    "    if (last_reward[-100:].mean() > goal_average_steps):\n",
    "        print (\"Win!\")\n",
    "        break\n",
    "env.close()\n",
    "# gym.upload('/tmp/cartpole-experiment-1', algorithm_id='simple Q-learning', api_key='your-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 15\n",
    "plt.plot(last_reward)\n",
    "plt.plot([last_reward[max(0, i-w):min(len(last_reward), i+w)].mean() for i in range(len(last_reward))])\n",
    "plt.legend(['score', 'RM-{}'.format(2*w)])\n",
    "plt.title('table-QN: alpha = {}, gamma = {}, epsilon = {}'.format(alpha, gamma, epsilon))\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "\n",
    "Если количество состояний велико, даже бесконечно ($~R^4$), то все запомнить не получится.\n",
    "\n",
    "Приходится использовать апроксимацию Q-функции.\n",
    "\n",
    "Например аппроксимацию нейронной сетью, т.е. $Q(s,a) = Q_{nn}(s,a|\\theta)$, где $\\theta$-веса сетки\n",
    "\n",
    "<img src=\"https://pp.userapi.com/c840129/v840129239/4afe4/R9TwZiy1LoA.jpg\" alt=\"MDP\" style=\"width: 600px;\"/>\n",
    "\n",
    "Есть только история $(s, a, r, s')$, полученная при следовании данной политике.\n",
    "\n",
    "Получим N таких значений и пересчитаем параметры $\\theta$, т.е. веса нейронной сети:\n",
    "\n",
    "$$Q_{target}(s,a) = r + gamma\\cdot \\max_{a'} Q(s',a' | \\theta)$$\n",
    "$$Loss(\\theta) = \\frac{1}{N} \\sum_{(s,a,r,s') \\in batch} \\left[Q_target(s,a) - Q(s,a | \\theta)\\right]^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ:\n",
    "    def __init__(self, inputs, outputs, mem_size, gamma, learning_rate, epsilon, batch_size):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "            - inputs: размерность входного вектора\n",
    "            - outputs: размерность выходного вектора (к-во действий)\n",
    "            - mem_size: размер буфера памяти\n",
    "            - gamma: параметр затухания награды\n",
    "            - learning_rate: множитель перед градиентом в SGD\n",
    "            - batch_size: размер подвыборки для тренировки\n",
    "        \"\"\"\n",
    "        self.actions = range(outputs)\n",
    "        self.epsilon = epsilon\n",
    "        self.input_size = inputs\n",
    "        self.output_size = outputs\n",
    "        self.memory = []\n",
    "        self.mem_size = mem_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.global_i = 0\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        hiddenLayers = [30,30]\n",
    "#         hiddenLayers = [30]\n",
    "        activationType = 'relu'\n",
    "        model.add(Dense(hiddenLayers[0], input_shape=(self.input_size,), init='lecun_uniform'))\n",
    "        model.add(Activation(activationType))\n",
    "\n",
    "        for index in range(1, len(hiddenLayers)):\n",
    "            model.add(Dense(hiddenLayers[index], init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "\n",
    "#       На выходе числа из R\n",
    "        model.add(Dense(self.output_size, init='lecun_uniform'))\n",
    "        model.add(Activation(\"linear\"))\n",
    "        \n",
    "#       Оптимизатор, чтобы устремлять Q -> Q_target\n",
    "        optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-06)\n",
    "    \n",
    "#       Ф-я потерь sum((Q[:]-Q_target[:])**2)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_Q_values(self, state):\n",
    "        predicted = self.model.predict(state.reshape(1,-1))\n",
    "        return predicted[0]\n",
    "    \n",
    "    def chooseAction(self, state):\n",
    "        q = self.get_Q_values(state).tolist()\n",
    "        maxQ = max(q)\n",
    "        \n",
    "        # Отвечает за баланс исследование-использование (exploration vs expluatation)\n",
    "        # С вероятностью epsilon случайное действие, 1-epsilon - argmax(Q)\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        \n",
    "        # Если максимум Q достигается для двух и более действий - берем любое\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            action = random.choice(best)\n",
    "        else:\n",
    "            action = q.index(maxQ)\n",
    "        return action\n",
    "    \n",
    "        # Пересчет значения Q(s,a) := r + gamma * max Q(s',a')\n",
    "    def calc_target(self, Q_values, reward, is_done):\n",
    "        if is_done:\n",
    "            return reward\n",
    "        else : \n",
    "            return reward + self.gamma * max(Q_values)\n",
    "    \n",
    "    def remember(self, sarsd):\n",
    "        if len(self.memory) < self.mem_size:\n",
    "            self.memory.append(sarsd)\n",
    "        else:\n",
    "            self.memory[self.global_i%self.mem_size] = sarsd\n",
    "        self.global_i += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        #выберем случайную подвыборку длины self.batch_size\n",
    "        batch_indexes = random.sample(range(len(self.memory)), self.batch_size)\n",
    "        \n",
    "        #контейнеры  X = вход; Y = ответ\n",
    "        X_batch = np.zeros((self.batch_size, self.input_size))\n",
    "        Y_batch = np.zeros((self.batch_size, self.output_size))\n",
    "        j = 0\n",
    "\n",
    "        for i in batch_indexes:\n",
    "        # выбираем из памяти s_t, a_t, r_t, s_{t+1} и метку конца \n",
    "            state, action, reward, new_state, is_done = self.memory[i]\n",
    "\n",
    "            # Q_target(s,a) := r + gamma * max Q(s',a')\n",
    "            Q_values = self.get_Q_values(state)\n",
    "            Q_values_new = self.get_Q_values(new_state)\n",
    "            Q_target = self.calc_target(Q_values_new, reward, is_done)\n",
    "\n",
    "            # Для всех a, кроме a_t не трогаем, для a_t меняем\n",
    "            X_batch[j] = np.array([state.copy()])\n",
    "            Y_sample = Q_values.copy()\n",
    "            Y_sample[action] = Q_target\n",
    "            Y_batch[j] = np.array([Y_sample])\n",
    "            \n",
    "            \n",
    "            if is_done:\n",
    "                X_batch[j] = np.array([new_state.copy()])\n",
    "                Y_batch[j] = np.array([[reward]*self.output_size])\n",
    "            j += 1\n",
    "            \n",
    "#       Оптимизатор устремляет Q -> Q_target\n",
    "#       т.е. делает 1 шаг против направления градиента ф-и потерь L = sum((Q[:]-Q_target[:])**2) по параметрам сетки\n",
    "        self.model.fit(X_batch, Y_batch, batch_size = self.batch_size, nb_epoch=1, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make('MsPacman-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "goal_average_steps = 475\n",
    "goal_number_of_steps = 500\n",
    "\n",
    "# env = gym.wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "mem_size = 1024\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00525\n",
    "epsilon = 0.1\n",
    "\n",
    "batch_size  = 256\n",
    "learn_step = 256\n",
    "last_reward = np.ndarray(0)\n",
    "state = env.reset().ravel()\n",
    "# Инициализация глубокого Q-learning\n",
    "qlearn = DeepQ(state.shape[0], env.action_space.n, mem_size, gamma, learning_rate, epsilon, batch_size)\n",
    "\n",
    "for i_episode in xrange(3000):\n",
    "    state = env.reset().ravel()\n",
    "    cum_reward = 0\n",
    "    for t in xrange(goal_number_of_steps):\n",
    "#         можно смотреть, как агент учится\n",
    "        if i_episode % 100 == 0: \n",
    "            env.render()\n",
    "\n",
    "        # в соответствии с политикой выберем действие\n",
    "        action = qlearn.chooseAction(state)\n",
    "        \n",
    "        # получим новое наблюдение, награду, метку конца эпизода\n",
    "        new_state, reward, is_done, info = env.step(action)\n",
    "        new_state = new_state.ravel()\n",
    "        cum_reward += reward\n",
    "        if not(is_done):\n",
    "            qlearn.remember((state, action, reward, new_state, is_done))\n",
    "            state = new_state\n",
    "        else:\n",
    "        # штраф за поражение\n",
    "            reward = -200\n",
    "            qlearn.remember((state, action, reward, new_state, is_done))\n",
    "            last_reward = np.append(last_reward, cum_reward)\n",
    "            break\n",
    "            \n",
    "    if qlearn.global_i > learn_step:\n",
    "#         print(qlearn.global_i)\n",
    "        qlearn.learn()\n",
    "    print(\"episode: {}, score: {}, 30-mean: {:0.1f}\".format(i_episode, t, last_reward[-30:].mean()))\n",
    "    if (last_reward[-100:].mean() > goal_average_steps):\n",
    "        print (\"Win!\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "# gym.upload('/tmp/cartpole-experiment-1', algorithm_id='vmayoral simple Q-learning', api_key='your-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 15\n",
    "plt.plot(last_reward)\n",
    "plt.plot([last_reward[max(0, i-w):min(len(last_reward), i+w)].mean() for i in range(len(last_reward))])\n",
    "plt.legend(['score', 'RM-{}'.format(2*w)])\n",
    "plt.title('DQN')\n",
    "plt.xlabel('step')\n",
    "plt.xlabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "0. Проследить зависимость результата от epsilon, построить графики для epsilon = [0.01, 0.07, 0.2, 0.5];\n",
    "0. Проследить зависимость результата от learning_rate, построить графики для разных learning_rate;\n",
    "0. Динамически менять epsilon и learning_rate, обучиться быстрее и стабильнее\n",
    "0. Усложнить модель, добавить слои, поменять число нейронов, ф-ию активации;\n",
    "0. $^*$ Набрать 1000 очков в [Ms.Packman](https://gym.openai.com/envs/MsPacman-v0/) в OpenAI.gym. Скорее всего понадобится добавить пару сверточных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За иллюстрации отдельное спасибо А.Гринчуку, см. [его семинар](rl_advantages.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
